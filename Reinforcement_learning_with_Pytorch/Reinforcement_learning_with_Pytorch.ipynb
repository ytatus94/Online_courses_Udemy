{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98ce8fe",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Pytorch\n",
    "\n",
    "* Stastics\n",
    "  * [CartPole statistics](#CartPole-statistics)\n",
    "  * [FrozenLake statistics](#FrozenLake-statistics)\n",
    "\n",
    "* Examples:\n",
    "  * [rl01_CartPoleRandom.py](#rl01_CartPoleRandom.py)\n",
    "    * [rl01_CartPoleRandom-code02.py](#rl01_CartPoleRandom-code02.py)\n",
    "  * [rl02_FrozenLakeRandom.py](#rl02_FrozenLakeRandom.py)\n",
    "    * [rl02_FrozenLakeRandom-code02.py](#rl02_FrozenLakeRandom-code02.py)\n",
    "  * [rl03_CartPoleVideo.py](#rl03_CartPoleVideo.py)\n",
    "  * [rl04_FrozenLakeStochasticDeterministic.py](#rl04_FrozenLakeStochasticDeterministic.py)\n",
    "    * [Stochastic 環境 & random action](#Stochastic-環境-&-random-action)\n",
    "    * [Deterministic 環境 & random action](#Deterministic-環境-&-random-action)\n",
    "  * [rl05_FrozenLakeDeterministicBellman.py](#rl05_FrozenLakeDeterministicBellman.py)\n",
    "  * [rl06_FrozenLakeStochastic.py](#rl06_FrozenLakeStochastic.py)\n",
    "    * [rl06-FrozenLake-0.4.0.py](#rl06-FrozenLake-0.4.0.py)\n",
    "  * [rl07_FrozenLakeStochasticQLearning.py](#rl07_FrozenLakeStochasticQLearning.py)\n",
    "  * [rl08_egreedy.py](#rl08_egreedy.py)\n",
    "    * [rl08-b-egreedy-decay.py](#rl08-b-egreedy-decay.py)\n",
    "  * [rl09_bonus_value_iteration.py](#rl09_bonus_value_iteration.py)\n",
    "  * [rl10_homework.py](#rl10_homework.py)\n",
    "  * [ rl11_NN_review.py](#rl11_NN_review.py)\n",
    "    * [rl11_NN-review-0.4.0.py](#rl11_NN-review-0.4.0.py)\n",
    "  * [rl12_CartPoleRandomNew.py](#rl12_CartPoleRandomNew.py)\n",
    "  * [rl13_egreedy_tool.py](#rl13_egreedy_tool.py)\n",
    "  * [rl14_CartPole-NN.py](#rl14_CartPole-NN.py)\n",
    "  * [rl15_CartPole-NN-log.py](#rl15_CartPole-NN-log.py)\n",
    "  * [rl16_CartPole-NN-2layer.py](#rl16_CartPole-NN-2layer.py)\n",
    "  * [rl17_CartPole-Challenge.py](#rl17_CartPole-Challenge.py)\n",
    "  * [rl18_CartPole-ExperienceReplay.py](#rl18_CartPole-ExperienceReplay.py)\n",
    "  * [rl19_CartPole-targetnet.py](#rl19_CartPole-targetnet.py)\n",
    "  * []()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdd0be",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "### CartPole statistics\n",
    "\n",
    "* [CartPole](https://gym.openai.com/envs/CartPole-v0/) 環境\n",
    "* 比較各種演算法的結果\n",
    "\n",
    "|演算法|程式碼|Average number of steps|Average reward|Average reward (last 100 episodes)|Solved after N episodes|\n",
    "|:---|:---|:---|:---|:---|:---|\n",
    "|Random moves|[`rl01_CartPoleRandom.py`](#rl01_CartPoleRandom.py)|22.30|\n",
    "|Random moves|[`rl12_CartPoleRandomNew.py`](#rl12_CartPoleRandomNew.py)||21.89|22.24|\n",
    "|NN - basic|[`rl14_CartPole-NN.py`](#rl14_CartPole-NN.py)||19.63|31.89|\n",
    "|NN - 2 layers|[`rl16_CartPole-NN-2layer.py`](#rl16_CartPole-NN-2layer.py)||152.56|188.79|\n",
    "|NN - 2 layers|[`rl17_CartPole-Challenge.py`](#rl17_CartPole-Challenge.py) 調整參數||173.81|200.00|\n",
    "|NN - 2 layers - Experience replay|[`rl18_CartPole-ExperienceReplay.py`](#rl18_CartPole-ExperienceReplay.py)||192.04|199.79|\n",
    "|NN - target net + tuning|[`rl19_CartPole-targetnet.py`](#rl19_CartPole-targetnet.py) 的第二組參數||187.46|199.37|\n",
    "|NN - Double DQN|[`rl20_CartPole-DoubleDQN.py`](#rl20_CartPole-DoubleDQN.py) 的第一組參數: 快||187.33|176.00|130|\n",
    "|NN - Double DQN|[`rl20_CartPole-DoubleDQN.py`](#rl20_CartPole-DoubleDQN.py) 的第二組參數: stable||103.97|200.00|382|\n",
    "|NN - Dueling DQN + tuning|quick-win||193.59|200.00|117|\n",
    "\n",
    "\n",
    "### FrozenLake statistics\n",
    "\n",
    "* 接下來要用 [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/) 的例子來說明與比較不同的 algorithms 的結果\n",
    "* 比較各種演算法的結果\n",
    "\n",
    "|演算法|程式碼|Percent of episodes finished successfully|Percent of episodes finished successfully (last 100 episodes)|Average number of steps|Average number of steps (last 100 episodes)|註解|\n",
    "|:---:|:---|:---|:---|:---|:---|:---|\n",
    "|Random|修改過的 [`rl02_FrozenLakeRandom.py`](#rl02_FrozenLakeRandom.py)|0.016|0.001|7.59|0.71|* stochastic 環境 & random action<br />* 只有少部分的 episode 順利結束|\n",
    "\n",
    "1. Random\n",
    "  * 用修改過的 [`rl02_FrozenLakeRandom.py`](#rl02_FrozenLakeRandom.py) 跑的結果\n",
    "    * stochastic 環境 & random action\n",
    "  * 只有少部分的 episode 順利結束\n",
    "```\n",
    "Percent of episodes finished successfully: 0.016\n",
    "Percent of episodes finished successfully (last 100 episodes): 0.001\n",
    "Average number of steps: 7.59\n",
    "Average number of steps (last 100 episodes): 0.71\n",
    "```\n",
    "2. Bellman equation (deterministic environment)\n",
    "  * 用 `rl05_FrozenLakeDeterministicBellman.py` 跑的結果\n",
    "    * deterministic 環境\n",
    "    * 用 Bellman equation 求 Q table: $Q(s, a) = r + \\gamma \\times \\max_{a'} Q(s', a')$\n",
    "  * 大部分的 episode 都順利結束了，表示 Bellman equation 可以得到很好的結果\n",
    "```\n",
    "Percentage of episodes finished successfully: 0.916\n",
    "Percentage of episodes finished successfully (last 100 episodes): 1.0\n",
    "Average number of steps: 6.22\n",
    "Average number of steps (last 100 episodes): 6.00\n",
    "```\n",
    "3. $\\epsilon$-greedy\n",
    "  * 用 `rl08_egreedy.py` 跑的結果\n",
    "    * deterministic 環境\n",
    "    * 用 Bellman equation 求 Q table: $Q(s, a) = r + \\gamma \\times \\max_{a'} Q(s', a')$\n",
    "    * 固定 $epsilon$ 的數值\n",
    "  * 大部分的 episode 都順利結束了，因為採用了 $\\epsilon$-greedy 所以有時候會探索未知，造成結果比單純用 Bellman equation 差一些\n",
    "```\n",
    "Percentage of episodes finished successfully: 0.713\n",
    "Percentage of episodes finished successfully (last 100 episodes): 0.85\n",
    "Average number of steps: 6.53\n",
    "Average number of steps (last 100 episodes): 6.35\n",
    "```\n",
    "4. Bellman equation (stochastic environment)\n",
    "  * 用 `rl06_FrozenLakeStochastic.py` 跑的結果\n",
    "    * stochastic 環境\n",
    "    * 用 Bellman equation 求 Q table: $Q(s, a) = r + \\gamma \\times \\max_{a'} Q(s', a')$\n",
    "  * 只有少部分的 episode 順利結束，表示 Bellman equation 不適合用在 stochastic 環境中\n",
    "```\n",
    "Percentage of episodes finished successfully: 0.017\n",
    "Percentage of episodes finished successfully (last 100 episodes): 0.02\n",
    "Average number of steps: 7.79\n",
    "Average number of steps (last 100 episodes): 7.88\n",
    "```\n",
    "5. Q-learning (stochastic environment)\n",
    "  * 用 `rl07_FrozenLakeStochasticQLearning.py` 跑的結果\n",
    "    * stochastic 環境\n",
    "    * 用 Q-learning 求 Q table: $Q(s, a) = (1 - \\alpha)Q(s, a) + \\alpha[r + \\gamma \\times \\max_{a'} Q(s', a')]$\n",
    "  * 大概有將近一半的 episode 順利結束，表示用 Q-learning 比用 Bellman equation 能得到更多的改善\n",
    "```\n",
    "Percentage of episodes finished successfully: 0.405\n",
    "Percentage of episodes finished successfully (last 100 episodes): 0.44\n",
    "Average number of steps: 39.26\n",
    "Average number of steps (last 100 episodes): 41.57\n",
    "```\n",
    "6. Bonus lesson\n",
    "  * Value iteration for stochastic environment\n",
    "  * 用 `rl09_bonus_value_iteration.py` 跑的結果\n",
    "    * stochastic 環境\n",
    "    * 用 value iteration 求 Q-table\n",
    "  * 有超過一半的 episode 順利結束，所以 Value iteration 又比用 Q-learning 好多了\n",
    "```\n",
    "Percentage of episodes finished successfully: 0.729\n",
    "Percentage of episodes finished successfully (last 100 episodes): 0.74\n",
    "Average number of steps: 40.80\n",
    "Average number of steps (last 100 episodes): 44.31\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f751b",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Leaner Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import WorldDemo\n",
    "import threading\n",
    "\n",
    "action = []\n",
    "actions = WorldDemo.actions\n",
    "\n",
    "def try_move(action):\n",
    "    if action == actions[0]:\n",
    "        WorldDemo.try_move(0, -1)\n",
    "    elif action == actions[1]:\n",
    "        WorldDemo.try_move(0, 1)\n",
    "    elif action == actions[2]:\n",
    "        WorldDemo.try_move(-1, 0)\n",
    "    elif action == actions[3]:\n",
    "        WorldDemo.try_move(1, 0)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "def main():\n",
    "    print(\"START\")\n",
    "    print(actions)\n",
    "    \n",
    "t = threading.Thread(target=main)\n",
    "t.daemon = True\n",
    "t.start()\n",
    "WorldDemo.start_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee771e9a",
   "metadata": {},
   "source": [
    "### rl01_CartPoleRandom.py\n",
    "\n",
    "* 採用 random action 的方式\n",
    "* 如何判斷一個 episode 結束:\n",
    "  * 就是要麻走完了 step 限制的 1000 步，要麻就是 CartPole 遊戲玩到死掉了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f7375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\") # 建立環境，這邊用 gym 中已經設定好的環境\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 最初要先 reset 狀態\n",
    "    for step in range(100): # 每一個 episode 最多走 100 步\n",
    "        action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        new_state, reward, done, info = env.step(action) # 告訴環境 agent 用了哪個 action\n",
    "        env.render() # 把圖案畫出來，會畫在新的 window 中，不會畫在 jupyter notebook 中\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a055b5",
   "metadata": {},
   "source": [
    "### rl01_CartPoleRandom-code02.py\n",
    "* 修改上面的程式碼，當結束的時候把總共走了幾個步驟記錄下來，然後畫圖。\n",
    "* 跑完的結果\n",
    "```\n",
    "Episode finised after 13 steps\n",
    "Average number of steps: 22.17\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fcdb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"CartPole-v1\") # 建立環境，這邊用 gym 中已經設定好的環境\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = [] # 用來儲存每個 episode 走了幾步\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 最初要先 reset 狀態\n",
    "    \n",
    "    step = 0 # 每個 episode 一開始的步數要先歸零\n",
    "    while True:\n",
    "        step += 1\n",
    "        action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        new_state, reward, done, info = env.step(action) # 告訴環境 agent 用了哪個 action\n",
    "\n",
    "        print(new_state) # 每走了一步之後變成新的狀態\n",
    "        print(info)\n",
    "\n",
    "        env.render() # 把圖案畫出來，會畫在新的 window 中，不會畫在 jupyter notebook 中\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step) # 把每個 episode 共走了幾步記錄下來\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "plt.plot(steps_total) # 把每個 episode 共走了幾步畫出來\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b29b798",
   "metadata": {},
   "source": [
    "### rl02_FrozenLakeRandom.py\n",
    "\n",
    "* 也是使用 random action 的方式\n",
    "* 但是不知道為什麼輸出和標示的 action 不同...\n",
    "  * 官網的說明: the ice is slippery, so you won't always move in the direction you intend.\n",
    "  * 所以真正的移動方向和 action 的方向可能不一樣 $\\Rightarrow$ 這是 stochastic 環境\n",
    "* 程式碼基本上和 [`rl01_CartPoleRandom.py`](#rl01_CartPoleRandom.py) 一樣，只是用不同的環境\n",
    "  * 用 `FrozenLake-v0` 環境\n",
    "  * 把 `rl01_CartPoleRandom.py` 中的 `print(new_state)` 和 `print(info)` 刪掉\n",
    "  * 加入 `time.sleep(0.4)`\n",
    "* 跑完的結果\n",
    "```\n",
    "Average number of steps: 7.69\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ccb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\") # 建立環境，這邊用 gym 中已經設定好的環境\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = [] # 用來儲存每個 episode 走了幾步\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 最初要先 reset 狀態\n",
    "    \n",
    "    step = 0 # 每個 episode 一開始的步數要先歸零\n",
    "    while True:\n",
    "        step += 1\n",
    "        action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        new_state, reward, done, info = env.step(action) # 告訴環境 agent 用了哪個 action\n",
    "        \n",
    "        time.sleep(0.4) # 每個 step 相隔 0.4 秒\n",
    "\n",
    "        env.render() # 把圖案畫出來，會畫在新的 window 中，不會畫在 jupyter notebook 中\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step) # 把每個 episode 共走了幾步記錄下來\n",
    "            print(\"Episode finished after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "plt.plot(steps_total) # 把每個 episode 共走了幾步畫出來\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d4df7",
   "metadata": {},
   "source": [
    "### rl02_FrozenLakeRandom-code02.py\n",
    "* 修改上面的程式碼，讓結果可以和其他的比較\n",
    "  * 要比較的有\n",
    "    * random move (就是下面這個程式)\n",
    "    * [deterministic](#Deterministic-環境-&-random-action)\n",
    "    * [stochastic](#Stochastic-環境-&-random-action)\n",
    "  * 加入 `rewards_total` 來記錄每個 episode 的 reward\n",
    "  * 把要輸出的資訊和圖表做些修改\n",
    "  * 跑完的結果\n",
    "```\n",
    "Percent of episodes finished successfully: 0.019\n",
    "Percent of episodes finished successfully (last 100 episodes): 0.0\n",
    "Average number of steps: 7.64\n",
    "Average number of steps (last 100 episodes): 7.84\n",
    "```\n",
    "  * 因為是 random walk, 所以只有很小一部分的 episodes 有完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acc70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\") # 建立環境，這邊用 gym 中已經設定好的環境\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = [] # 用來儲存每個 episode 走了幾步\n",
    "rewards_total = [] # 用來記錄每個 episode 的 reward\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 最初要先 reset 狀態\n",
    "    \n",
    "    step = 0 # 每個 episode 一開始的步數要先歸零\n",
    "    while True:\n",
    "        step += 1\n",
    "        action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        new_state, reward, done, info = env.step(action) # 告訴環境 agent 用了哪個 action\n",
    "        \n",
    "        # time.sleep(0.4) # 每個 step 相隔 0.4 秒\n",
    "\n",
    "        # env.render() # 把圖案畫出來，會畫在新的 window 中，不會畫在 jupyter notebook 中\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step) # 把每個 episode 共走了幾步記錄下來\n",
    "            rewards_total.append(reward)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Percent of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percent of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c5b650",
   "metadata": {},
   "source": [
    "### rl03_CartPoleVideo.py\n",
    "* 這個例子是用來說明如何記錄 moves (i.e. agent behaves)\n",
    "* 要有安裝 ffmpeg\n",
    "  * On OS X, you can install ffmpeg via `brew install ffmpeg`.\n",
    "  * On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. \n",
    "    * On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\n",
    "* 修改 [`rl01_CartPoleRandom.py`](#rl01_CartPoleRandom) 的第二支程式，加入下面兩行\n",
    "```python\n",
    "videosDir = \"./RLvideos/\" # 指定哪邊存放 videos\n",
    "env = gym.wrappers.Monitor(env, videosDir) # 要把結果錄下來，就要這一行\n",
    "```\n",
    "* 跑完的結果\n",
    "```\n",
    "Average number of steps: 21.36\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"CartPole-v1\") # 建立環境，這邊用 gym 中已經設定好的環境\n",
    "\n",
    "videosDir = \"./RLvideos/\" # 指定哪邊存放 videos\n",
    "env = gym.wrappers.Monitor(env, videosDir) # 要把結果錄下來，就要這一行\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = [] # 用來儲存每個 episode 走了幾步\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 最初要先 reset 狀態\n",
    "    \n",
    "    step = 0 # 每個 episode 一開始的步數要先歸零\n",
    "    while True:\n",
    "        step += 1\n",
    "        action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        new_state, reward, done, info = env.step(action) # 告訴環境 agent 用了哪個 action\n",
    "\n",
    "        env.render() # 把圖案畫出來，會畫在新的 window 中，不會畫在 jupyter notebook 中\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step) # 把每個 episode 共走了幾步記錄下來\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "plt.plot(steps_total) # 把每個 episode 共走了幾步畫出來\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318dffb",
   "metadata": {},
   "source": [
    "* 列出 site packages 存放的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab1b98b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/local/anaconda3/lib/python3.8/site-packages']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import site\n",
    "site.getsitepackages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f242ceb",
   "metadata": {},
   "source": [
    "### rl04_FrozenLakeStochasticDeterministic.py\n",
    "\n",
    "* 原本的 [`rl02_FrozenLakeRandom.py`](#rl02_FrozenLakeRandom.py) 就是 stochastic 的範例\n",
    "  * stochastic 就是說想要執行的動作與真正執行得動作不一定是一樣的，只有一定的機率會去執行想要執行的動作\n",
    "  * 例如下圖，原本在 S 想要採取 action = Right 但是真正執行這個 action 的機率只有 0.33\n",
    "```\n",
    "  (Right)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "0\n",
    "{'prob': 0.3333333333333333}\n",
    "```\n",
    "\n",
    "### Stochastic 環境 & random action\n",
    "* 程式碼基本上就是和 [`rl02_FrozenLakeRandom.py`](#rl02_FrozenLakeRandom.py) 一模一樣，只是加入了兩行輸出\n",
    "```python\n",
    "print(new_state) # 每走了一步之後變成新的狀態\n",
    "print(info) # info 可以顯示真正執行想要的動作的機率\n",
    "```\n",
    "* 跑完的結果\n",
    "```\n",
    "Average number of steps: 7.49\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0140ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\") # 建立環境，這邊用 gym 中已經設定好的環境\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = [] # 用來儲存每個 episode 走了幾步\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 最初要先 reset 狀態\n",
    "\n",
    "    step = 0 # 每個 episode 一開始的步數要先歸零\n",
    "    while True:\n",
    "        step += 1\n",
    "        action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        new_state, reward, done, info = env.step(action) # 告訴環境 agent 用了哪個 action\n",
    "        \n",
    "        time.sleep(0.4) # 每個 step 相隔 0.4 秒\n",
    "\n",
    "        env.render() # 把圖案畫出來，會畫在新的 window 中，不會畫在 jupyter notebook 中\n",
    "\n",
    "        print(new_state) # 每走了一步之後變成新的狀態\n",
    "        print(info) # info 可以顯示真正執行想要的動作的機率\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step) # 把每個 episode 共走了幾步記錄下來\n",
    "            print(\"Episode finished after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "plt.plot(steps_total) # 把每個 episode 共走了幾步畫出來\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d3747",
   "metadata": {},
   "source": [
    "### Deterministic 環境 & random action\n",
    "* Deterministic 的話要對程式做修改\n",
    "  * Deterministic 就是說想要執行的動作，和真正執行的動作，是一樣的\n",
    "  * 例如下圖，本來在 S 想要執行 action = Down，因為真正執行這個 action 的機率是 1，所以結束的狀態是 F\n",
    "```\n",
    "  (Down)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "4\n",
    "{'prob': 1.0}\n",
    "```\n",
    "* Deterministic 的程式碼和 Stochastic 的程式碼差別在使用的環境不一樣\n",
    "  * Deterministic 用下面的環境設定\n",
    "```python\n",
    "# 要 deterministic 的話要加入這一部分\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id=\"FrozenLakeNotSlippery-v0\",\n",
    "    entry_point=\"gym.envs.toy_text:FrozenLakeEnv\",\n",
    "    kwargs={\"map_name\": \"4x4\", \"is_slippery\": False},\n",
    "#     max_episode_steps=100,\n",
    "#     reward_threshold=0.78, # optimum = .8196\n",
    ")\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\") # Deterministic 要用不同的環境\n",
    "```\n",
    "* 跑完的結果\n",
    "```\n",
    "Average number of steps: 7.92\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bdad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 要 deterministic 的話要加入這一部分\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id=\"FrozenLakeNotSlippery-v0\",\n",
    "    entry_point=\"gym.envs.toy_text:FrozenLakeEnv\",\n",
    "    kwargs={\"map_name\": \"4x4\", \"is_slippery\": False},\n",
    "#     max_episode_steps=100,\n",
    "#     reward_threshold=0.78, # optimum = .8196\n",
    ")\n",
    "\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\") # Deterministic 要用不同的環境\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = [] # 用來儲存每個 episode 走了幾步\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 最初要先 reset 狀態\n",
    "\n",
    "    step = 0 # 每個 episode 一開始的步數要先歸零\n",
    "    while True:\n",
    "        step += 1\n",
    "        action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        new_state, reward, done, info = env.step(action) # 告訴環境 agent 用了哪個 action\n",
    "        \n",
    "        time.sleep(0.4) # 每個 step 相隔 0.4 秒\n",
    "\n",
    "        env.render() # 把圖案畫出來，會畫在新的 window 中，不會畫在 jupyter notebook 中\n",
    "\n",
    "        print(new_state) # 每走了一步之後變成新的狀態\n",
    "        print(info) # info 可以顯示真正執行想要的動作的機率\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step) # 把每個 episode 共走了幾步記錄下來\n",
    "            print(\"Episode finished after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "plt.plot(steps_total) # 把每個 episode 共走了幾步畫出來\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd1169",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "* Agent 要把 total future rewards 最大化\n",
    "  * Deterministic: $R_{t} = r_{t} + r_{t+1} + r_{t+2} + \\cdots + r_{n}$\n",
    "  * Stochastic: $R_{t} = r_{t} + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots + \\gamma^{n-t} r_{n}$\n",
    "    * 當對未來有不確定性的時候，rewards 要引入 discount factor $\\gamma$，使得越遠的未來的 reward 越不重要\n",
    "    * $0 < \\gamma < 1$\n",
    "      * $\\gamma=0$: only immediately reward\n",
    "      * $\\gamma=1$: deterministic\n",
    "      * 通常用 $\\gamma=0.99$ or 0.9 之類的數值\n",
    "\n",
    "## Markov Decision Process\n",
    "* $V(s) := \\sum_{s'} P_{\\pi(s)}(s, s')\\Big(R_{\\pi(s)}(s, s') + \\gamma V(s')\\Big)$\n",
    "* $\\pi(s) := argmax_{a} \\Big\\{\\sum_{s'} P_{a}(s, s')\\Big(R_{a}(s, s') + \\gamma V(s')\\Big)\\Big\\}$\n",
    "* $V_{i+1}(s) := \\max_{a}\\Big\\{\\sum_{s'}P_{a}(s, s')\\Big(R_{a}(s, s') + \\gamma V_{i}(s')\\Big)\\Big\\}$\n",
    "  * stochastic environment 中要把所有可能的結果都列入考慮，所以要把每個可能的結果的幾率加入公式裡面\n",
    "* $Q(s, a) = \\sum_{s'} P_{a}(s, s')\\Big(R_{a}(s, s') + \\gamma V(s')\\Big)$\n",
    "\n",
    "## Solution\n",
    "* $Q$-value function: $Q^{\\pi}(s, a) = \\mathbb{E}\\Big[\\sum_{t \\ge 0}\\gamma^{t} r_{t}\\Big| s_{0}=s, a_{0}=a, \\pi\\Big]$\n",
    "  * $\\pi$ 是 policy, $\\mathbb{E}$ 是期望值\n",
    "  * $Q$-value function 可以想成是某一種 quality function，用來判斷 state-action pair 的好壞\n",
    "    * 在某個 policy $\\pi$ 的情況下，某個 state 執行某個 action 的期望值\n",
    "* 最佳化的 $Q$-value: $Q^{*}(s, a) = \\max_{\\pi}\\mathbb{E}\\Big[\\sum_{t \\ge 0} \\gamma^{t} r_{t} \\Big| s_{0}=s, a_{0}=a, \\pi\\Big]$\n",
    "* $Q^{*}(s, a) = \\mathbb{E}_{s'\\sim \\epsilon}\\Big[r + \\gamma \\max_{a'} Q^{*}(s', a')\\Big|s, a\\Big]$\n",
    "\n",
    "## Bellman equation\n",
    "* $Q(s, a) = r + \\gamma \\max_{a'}Q(s', a')$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e2517",
   "metadata": {},
   "source": [
    "* 印出 action space 和 observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba4cb377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e888e165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4317f301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f2c40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd25600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ff23b",
   "metadata": {},
   "source": [
    "* Tabular method 就是建立一個大大的表格，每個 row 代表一個 state 每個 column 代表一個 action，每一格的數值就是 $Q$-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a50c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "number_of_states = env.observation_space.n\n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "235aa505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd4824",
   "metadata": {},
   "source": [
    "因為環境有 16 個 state 和 4 個 action，所以 Q table 就有 16 rows x 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57279944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(1, 4)\n",
    "a # 是一個 1x4 的 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a7e52ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] # 是 size 1 的 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e0c134e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0], a[0][1] # 取得數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f74a2487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2000, 0.6000, 0.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][1] = 0.2 # 修改數值\n",
    "a[0][2] = 0.6\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68a71405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.6000]),\n",
       "indices=tensor([2]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, 1)\n",
    "# 第二個參數指明怎麼找最大值\n",
    "# 1: find the max value in the row\n",
    "# 0: find the max value in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f0415b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, 1)[0] # get the max value in tensor form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a17b3f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, 1)[1] # get index of the max value，傳回的是 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "373fca60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, 1)[0][0] # get the max value in value form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dcf888d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, 1)[1][0] # 沿著 row 方向上，數值最大的那個的 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a34b234e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # 隨機的 action 所以每次的結果都不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f912064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # 隨機的 action 所以每次的結果都不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc860649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.zeros(1, 4)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2e94526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.]),\n",
       "indices=tensor([0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(b, 1) # 當 tensor 中的元素都一樣的時候，torch.max() 傳回最左邊的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85ed39c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.]),\n",
       "indices=tensor([0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(b, 1) # 當 tensor 中的元素都一樣的時候，torch.max() 傳回最左邊的元素\n",
    "# 不管跑幾次都是一樣只傳回最左邊的元素，所以失去了隨機性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c18a974a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1938,  0.8813,  1.4464, -0.8650]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 4) # 產生 1x4 的隨機數值 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "498004b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4518, 0.0544, 0.6908, 0.7725]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 4) # 產生 1x4 的隨機數值 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00edd798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.3475e-05, 1.6591e-08, 1.2601e-04, 8.4253e-04]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 4)/1000 # 產生 1x4 的隨機數值 tensor，除以 1000 讓這個隨機數值變得很小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e1df5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0014, -0.0012,  0.0008, -0.0022])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(1, 4)/1000)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "881eb201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0003)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(1, 4)/1000)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab247856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[0] # Q table 的內容都是 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb27c5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0007, -0.0002,  0.0002,  0.0006]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[0] + torch.randn(1, 4)/1000 # Q table 的內容是很接近 0 的一個很小的數"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db224323",
   "metadata": {},
   "source": [
    "### rl05_FrozenLakeDeterministicBellman.py\n",
    "* 用 deterministic 環境\n",
    "* 用 Bellman equation 求 $Q$-table\n",
    "  * $Q(s, a) = r + \\gamma \\max_{a'}Q(s', a')$\n",
    "\n",
    "## Algorithm for deterministic environment\n",
    "```\n",
    "initial Q[num_states, num_actions]\n",
    "observe initial state s\n",
    "repeat until terminated\n",
    "    select and perform action a\n",
    "    observe reward r and new state s'\n",
    "    Q(s, a) = r + gamma * max Q(s', a')\n",
    "    s = s'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 要 deterministic 的話要加入這一部分\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id=\"FrozenLakeNotSlippery-v0\",\n",
    "    entry_point=\"gym.envs.toy_text:FrozenLakeEnv\",\n",
    "    kwargs={\"map_name\": \"4x4\", \"is_slippery\": False},\n",
    ")\n",
    "\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "gamma = 1 # deterministic\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        # action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        # action = torch.max(Q[state], 1)[1][0] # 沿著 row 方向找最大值，[1] 是第一個元素表示 index 結果是 tensor 所以要用 [0] 取得數值\n",
    "        # 上面的 action 公式雖然正確，可是因為一開始的 Q table 都是 0 所以上面的公式只會選出第一個 action，因此失去的 exploration\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        Q[state, action] = reward + gamma * torch.max(Q[new_state]) # 6. Q(s, a) = r + gamma * max Q(s', a')\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "plt.plot(steps_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15882b50",
   "metadata": {},
   "source": [
    "修改上面的程式碼，印出更多的資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c09923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 要 deterministic 的話要加入這一部分\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id=\"FrozenLakeNotSlippery-v0\",\n",
    "    entry_point=\"gym.envs.toy_text:FrozenLakeEnv\",\n",
    "    kwargs={\"map_name\": \"4x4\", \"is_slippery\": False},\n",
    ")\n",
    "\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "gamma = 1 # deterministic\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        # action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        # action = torch.max(Q[state], 1)[1][0] # 沿著 row 方向找最大值，[1] 是第一個元素表示 index 結果是 tensor 所以要用 [0] 取得數值\n",
    "        # 上面的 action 公式雖然正確，可是因為一開始的 Q table 都是 0 所以上面的公式只會選出第一個 action，因此失去的 exploration\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        Q[state, action] = reward + gamma * torch.max(Q[new_state]) # 6. Q(s, a) = r + gamma * max Q(s', a')\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.plot(steps_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dea678",
   "metadata": {},
   "source": [
    "增加畫 reward 的圖的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01311a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 要 deterministic 的話要加入這一部分\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id=\"FrozenLakeNotSlippery-v0\",\n",
    "    entry_point=\"gym.envs.toy_text:FrozenLakeEnv\",\n",
    "    kwargs={\"map_name\": \"4x4\", \"is_slippery\": False},\n",
    ")\n",
    "\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "gamma = 1 # deterministic\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        # action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        # action = torch.max(Q[state], 1)[1][0] # 沿著 row 方向找最大值，[1] 是第一個元素表示 index 結果是 tensor 所以要用 [0] 取得數值\n",
    "        # 上面的 action 公式雖然正確，可是因為一開始的 Q table 都是 0 所以上面的公式只會選出第一個 action，因此失去的 exploration\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        Q[state, action] = reward + gamma * torch.max(Q[new_state]) # 6. Q(s, a) = r + gamma * max Q(s', a')\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(reward)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.plot(steps_total)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rewards_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539d433",
   "metadata": {},
   "source": [
    "最後修改畫圖的部分，用 bar plot 來畫得好看一點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 要 deterministic 的話要加入這一部分\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id=\"FrozenLakeNotSlippery-v0\",\n",
    "    entry_point=\"gym.envs.toy_text:FrozenLakeEnv\",\n",
    "    kwargs={\"map_name\": \"4x4\", \"is_slippery\": False},\n",
    ")\n",
    "\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "gamma = 1 # deterministic\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "print(\"Initial Q-table:\\n\", Q)\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        # action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        # action = torch.max(Q[state], 1)[1][0] # 沿著 row 方向找最大值，[1] 是第一個元素表示 index 結果是 tensor 所以要用 [0] 取得數值\n",
    "        # 上面的 action 公式雖然正確，可是因為一開始的 Q table 都是 0 所以上面的公式只會選出第一個 action，因此失去的 exploration\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        Q[state, action] = reward + gamma * torch.max(Q[new_state]) # 6. Q(s, a) = r + gamma * max Q(s', a')\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(reward)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3aaf6",
   "metadata": {},
   "source": [
    "### rl06_FrozenLakeStochastic.py\n",
    "* 用 stochastic 環境\n",
    "* 用 Bellman equation 求 Q-table\n",
    "  * $Q(s, a) = r + \\gamma \\max_{a'}Q(s', a')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6e464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "gamma = 1 # deterministic\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        # action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        # action = torch.max(Q[state], 1)[1][0] # 沿著 row 方向找最大值，[1] 是第一個元素表示 index 結果是 tensor 所以要用 [0] 取得數值\n",
    "        # 上面的 action 公式雖然正確，可是因為一開始的 Q table 都是 0 所以上面的公式只會選出第一個 action，因此失去的 exploration\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        Q[state, action] = reward + gamma * torch.max(Q[new_state]) # 6. Q(s, a) = r + gamma * max Q(s', a')\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(reward)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(steps_total)\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(rewards_total)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7f5ab",
   "metadata": {},
   "source": [
    "### rl06-FrozenLake-0.4.0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n\n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions])\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    # for step in range(100)\n",
    "    while True:\n",
    "        step += 1\n",
    "        # action = env.action_space.sample()\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1][0]\n",
    "        new_state, reward, done, info = env.step(action.item())\n",
    "        Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        state = new_state\n",
    "        # time.sleep(0.4)\n",
    "        # env.render()\n",
    "        # print(new_state)\n",
    "        # print(info)\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(reward)\n",
    "            print(\"Episode finished after %i steps\" % step)\n",
    "            \n",
    "print(Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e2d00",
   "metadata": {},
   "source": [
    "## Temporal difference\n",
    "* The different between the current observation and the previous observation\n",
    "* $[r + \\gamma \\times \\max_{a'}Q(s', a')] - [Q(s, a)]$\n",
    "  * current observation: $r + \\gamma \\times \\max_{a'}Q(s', a')$\n",
    "  * previous observation: $Q(s, a)$\n",
    "  \n",
    "## Q-learning\n",
    "* Q-learning is off policy temporal difference control algorithm\n",
    "* $Q_{t}(s, a) = Q_{t-1}(s, a) + \\alpha \\times TD$\n",
    "  * $\\alpha$: learning rate\n",
    "* $Q(s, a) = Q(s, a) + \\alpha [r + \\gamma \\times \\max_{a'}Q(s', a') - Q(s, a)]$\n",
    "* $Q(s, a) = (1 - \\alpha)Q(s, a) + \\alpha[r + \\gamma \\times \\max_{a'}Q(s', a')]$\n",
    "  * $\\alpha=0$: we only care about the experience\n",
    "  * $\\alpha=1$: we don't care about the experience, we only care about the current observation\n",
    "  \n",
    "## Algorithm for stochastic environment\n",
    "```\n",
    "initial Q[num_states, num_actions]\n",
    "observe initial state s\n",
    "repeat until terminated\n",
    "    select and perform action a\n",
    "    observe reward r and new state s'\n",
    "    Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]\n",
    "    s = s'\n",
    "```\n",
    "\n",
    "* Deterministic 和 stochastic environments 的 algorithm 唯一的差別在於如何計算 $Q(s, a)$\n",
    "  * deterministic: $Q(s, a) = r + \\gamma \\times \\max_{a'} Q(s', a')$\n",
    "  * stochastic: $Q(s, a) = (1 - \\alpha)Q(s, a) + \\alpha [ r + \\gamma \\times \\max_{a'}Q(s', a')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b19ff",
   "metadata": {},
   "source": [
    "### rl07_FrozenLakeStochasticQLearning.py\n",
    "* 用 stochastic 環境\n",
    "* 用 Q-learning 來計算 Q-table\n",
    "  * Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# 要調整這兩個數值，以得到最合適的\n",
    "gamma = 0.95 # 因為不是用 deterministic environment 所以 gamma 要比 1 小\n",
    "learning_rate = 0.9 # learning rate 是要實驗才知道哪個數值合適，要在現在的觀察結果與過去的經驗值之間取得平衡\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "print(\"Initial Q-table:\\n\", Q)\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        # action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        # action = torch.max(Q[state], 1)[1][0] # 沿著 row 方向找最大值，[1] 是第一個元素表示 index 結果是 tensor 所以要用 [0] 取得數值\n",
    "        # 上面的 action 公式雖然正確，可是因為一開始的 Q table 都是 0 所以上面的公式只會選出第一個 action，因此失去的 exploration\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        # 6. Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]\n",
    "        # 這裡的 Q 值計算，和 rl06_FrozenLakeStochastic.py 不同\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] \\\n",
    "                           + learning_rate * (reward + gamma * torch.max(Q[new_state]))\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(reward)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9354e2",
   "metadata": {},
   "source": [
    "## Exploitation vs Exploration\n",
    "\n",
    "* 在 best decision 和 more information 之間取得一個平衡\n",
    "  * best decision 就是用 knowledge about the environment 來做的\n",
    "* $\\epsilon$-greedy: 設定一個 $\\epsilon$ 數值，然後把機率和這個數值比較\n",
    "  * $a = 1 - \\epsilon$\n",
    "  * $a = \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9451f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2946])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ab724d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7380])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "841279d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5711)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5345d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1910)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75a820",
   "metadata": {},
   "source": [
    "### rl08_egreedy.py\n",
    "* 用 deterministic 環境\n",
    "* 用 Bellman equation 求 Q table\n",
    "  * $Q(s, a) = r + \\gamma \\times \\max_{a'}Q(s', a')$\n",
    "* 用 $\\epsilon$-greedy 來做 exploitation and exploration\n",
    "  * 固定 $\\epsilon$ 的數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 要 deterministic 的話要加入這一部分\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id=\"FrozenLakeNotSlippery-v0\",\n",
    "    entry_point=\"gym.envs.toy_text:FrozenLakeEnv\",\n",
    "    kwargs={\"map_name\": \"4x4\", \"is_slippery\": False},\n",
    ")\n",
    "\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# gamma = 1 # deterministic\n",
    "gamma = 0.9\n",
    "egreedy = 0.1 # epsilon greedy = 0.1 表示有十分之一的機會會採取 random action\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "print(\"Initial Q-table:\\n\", Q)\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        random_for_egreedy = torch.rand(1)[0] # 產生一個介於 0 和 1 之間的隨機數，然後和 egreedy 比較\n",
    "        if random_for_egreedy > egreedy:\n",
    "        # action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        # action = torch.max(Q[state], 1)[1][0] # 沿著 row 方向找最大值，[1] 是第一個元素表示 index 結果是 tensor 所以要用 [0] 取得數值\n",
    "        # 上面的 action 公式雖然正確，可是因為一開始的 Q table 都是 0 所以上面的公式只會選出第一個 action，因此失去的 exploration\n",
    "            random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "            action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        Q[state, action] = reward + gamma * torch.max(Q[new_state]) # 6. Q(s, a) = r + gamma * max Q(s', a')\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(reward)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(steps_total)\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(rewards_total)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b554f6",
   "metadata": {},
   "source": [
    "* 上面的例子是固定 $\\epsilon$ 的值，但是也可以在訓練模型時用會改變的 $\\epsilon$\n",
    "  * 因為一開始我們對環境的認識並不多，所以 action 會是比較 random 的 (Exploitation)\n",
    "  * 隨著我們對環境越加了解後，我們就更該利用我們知道的 (Exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f629a",
   "metadata": {},
   "source": [
    "### rl08-b-egreedy-decay.py\n",
    "* 用 deterministic 環境\n",
    "* 用 Bellman equation 求 Q table\n",
    "  * $Q(s, a) = r + \\gamma \\times \\max_{a'}Q(s', a')$\n",
    "* 用 $\\epsilon$-greedy 來做 exploitation and exploration\n",
    "  * $\\epsilon$ 會改變"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859332b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 要 deterministic 的話要加入這一部分\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id=\"FrozenLakeNotSlippery-v0\",\n",
    "    entry_point=\"gym.envs.toy_text:FrozenLakeEnv\",\n",
    "    kwargs={\"map_name\": \"4x4\", \"is_slippery\": False},\n",
    ")\n",
    "\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# gamma = 1 # deterministic\n",
    "gamma = 0.9\n",
    "\n",
    "egreedy = 0.7 # 一開始的 epsilon 數值會很大，然後會漸漸變小\n",
    "egreedy_final = 0.1\n",
    "egreedy_decay = 0.999\n",
    "\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "print(\"Initial Q-table:\\n\", Q)\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "egreedy_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > egreedy:\n",
    "        # action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        # action = torch.max(Q[state], 1)[1][0] # 沿著 row 方向找最大值，[1] 是第一個元素表示 index 結果是 tensor 所以要用 [0] 取得數值\n",
    "        # 上面的 action 公式雖然正確，可是因為一開始的 Q table 都是 0 所以上面的公式只會選出第一個 action，因此失去的 exploration\n",
    "            random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "            action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        if egreedy > egreedy_final:\n",
    "            egreedy *= egreedy_decay\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        Q[state, action] = reward + gamma * torch.max(Q[new_state]) # 6. Q(s, a) = r + gamma * max Q(s', a')\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(reward)\n",
    "            egreedy_total.append(egreedy)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Egreedy value\")\n",
    "plt.bar(torch.arange(len(egreedy_total)), egreedy_total, alpha=0.6, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee3b4c",
   "metadata": {},
   "source": [
    "## Value iteration\n",
    "* Policy consists of clear instructions to Agent how to behave in our env\n",
    "* Offline planning: agent 在選擇 action 之前，就對環境很了解，知道選擇了哪一個 ation 之後會有怎樣的結果\n",
    "  * Agent 知道全部的 actions 執行後的結果\n",
    "* 目標是建立一個表格，有每個 state 和對應的 value\n",
    "  * 之前的方法是 agent 對環境不了解，藉由 exploration 慢慢地建立表格，並藉由學習把數值更新變得更好\n",
    "    * 先建立表格，再藉由探索以學習慢慢更新，得到好結果\n",
    "  * 現在的方法是對所有可能的 state 做很多次迭代，以得到最好的結果，這樣就能知道選擇哪個 action 比較合適\n",
    "    * 先建立表格，直接用迭代，來得到最可能的好結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f8c99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a712173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)]},\n",
       " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)]},\n",
       " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)]},\n",
       " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.P # 輸出每個 state 允許的動作，以及執行該動作的機率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d6abae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 12, 0.0, True)],\n",
       " 1: [(0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 12, 0.0, True),\n",
       "  (0.3333333333333333, 9, 0.0, False)],\n",
       " 2: [(0.3333333333333333, 12, 0.0, True),\n",
       "  (0.3333333333333333, 9, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 9, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False)]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.P[8] # 在 state 8 有 4 種 moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "779bf998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 8, 0.0, False),\n",
       " (0.3333333333333333, 12, 0.0, True),\n",
       " (0.3333333333333333, 9, 0.0, False)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.P[8][1] # 在 state 8 選擇動作 1 的時候，有三種可能的結果\n",
    "# 每個 tuple 表示每種結果的 (機率，動作結束後會在哪個 state，reward，是否結束 episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0db2cd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = torch.zeros([number_of_states])\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ca05d",
   "metadata": {},
   "source": [
    "### rl09_bonus_value_iteration.py\n",
    "* 用 Stochastic 環境\n",
    "* 用 Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea68027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# stochastic\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "number_of_states = env.observation_space.n\n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# V value - size 16 (as number of fields[states] in our lake)\n",
    "V = torch.zeros([number_of_states])\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "rewards_total = []\n",
    "steps_total = []\n",
    "\n",
    "# this is common function used in value_iteration and bulid_policy\n",
    "# It gets through all possible moves from defined state\n",
    "# and it returns best possible move (its value and index)\n",
    "def next_step_evaluation(state, Vvalues):\n",
    "    Vtemp = torch.zeros(number_of_actions)\n",
    "    \n",
    "    for action_possible in range(number_of_actions): # 把某個狀態下所有可能的 action 都做一遍\n",
    "        for prob, new_state, reward, _ in env.env.P[state][action_possible]:\n",
    "            Vtemp[action_possible] += prob * (reward + gamma * Vvalues[new_state])\n",
    "            \n",
    "    max_value, indice = torch.max(Vtemp, 0)\n",
    "    return max_value, indice\n",
    "\n",
    "# VALUE ITERATION\n",
    "# this will build V values table from scratch\n",
    "# will go through all possible states\n",
    "def value_iteration():\n",
    "    Qvalues = torch.zeros(number_of_states)\n",
    "    # this is value based on experiment\n",
    "    # after that many iterations values don't change significantly any more\n",
    "    # it can be done in better way - with some kind of evaluation of our values\n",
    "    # but this is simplified version which works also well in this example\n",
    "    max_iterations = 1500\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        # for each step we search for best possible move\n",
    "        for state in range(number_of_states):\n",
    "            max_value, _ = next_step_evaluation(state, Qvalues)\n",
    "            # Qvalues[state] = max_value[0] # 舊版的 pytorch\n",
    "            Qvalues[state] = max_value\n",
    "            \n",
    "    return Qvalues\n",
    "\n",
    "# BUILD POLICY\n",
    "# Now having V table - we can use it to build policy\n",
    "# policy means clear instructions which are best moves from each single state\n",
    "# So having V values table ready - we can easily understand which move is the best\n",
    "# in each step\n",
    "# so we are able to build clear instructions for our agent\n",
    "# telling him which move he should choose in every state\n",
    "def build_policy(Vvalues):\n",
    "    Vpolicy = torch.zeros(number_of_states)\n",
    "    \n",
    "    for state in range(number_of_states):\n",
    "        _, indice = next_step_evaluation(state, Vvalues)\n",
    "        # Vpolicy[state] = indice[0] # 舊版的 pytorch\n",
    "        Vpolicy[state] = indice\n",
    "        \n",
    "    return Vpolicy\n",
    "\n",
    "# 2 main steps to build policy for our agent\n",
    "V = value_iteration() # 用 value iteration 建立一個 V 表格\n",
    "Vpolicy = build_policy(V) # 由 V 表格來查看哪個 action 是最好的選擇\n",
    "\n",
    "# import sys\n",
    "# print(\"V:\", V)\n",
    "# print(\"Vpolicy:\", Vpolicy)\n",
    "# sys.exit()\n",
    "\n",
    "# main loop for our target\n",
    "num_episodes = 1000\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        \n",
    "        # action = Vpolicy[state]\n",
    "        action = int(Vpolicy[state]) # Vpolicy[state] 傳回的是 float 但是 action 要用 int\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            rewards_total.append(reward)\n",
    "            steps_total.append(step)\n",
    "            break\n",
    "            \n",
    "print(V)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7d9c8",
   "metadata": {},
   "source": [
    "### rl10_homework.py\n",
    "\n",
    "* 雖然 taxi-v2 是 deterministic 的，但是這邊試著用 stochastic 的 algorithm 來算 Q 值，試看看這樣的結果有沒有比較好\n",
    "* Stochastic: $Q(s, a) = (1 - \\alpha)Q(s, a) + \\alpha [ r + \\gamma \\times \\max_{a'}Q(s', a')]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc515041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "number_of_states = env.observation_space.n\n",
    "number_of_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9aeffef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8db2df06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_actions # 上, 下, 左, 右, pick up, drop off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# env = gym.make(\"Taxi-v2\") # 舊版本\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# 要調整這兩個數值，以得到最合適的\n",
    "gamma = 0.95 # 因為不是用 deterministic environment 所以 gamma 要比 1 小\n",
    "learning_rate = 0.9 # learning rate 是要實驗才知道哪個數值合適，要在現在的觀察結果與過去的經驗值之間取得平衡\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        # 6. Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]\n",
    "        # 這裡的 Q 值計算，和 rl06_FrozenLakeStochastic.py 不同\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] \\\n",
    "                           + learning_rate * (reward + gamma * torch.max(Q[new_state]))\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(score)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            print(score)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ecbb10",
   "metadata": {},
   "source": [
    "改善畫圖的部分，只畫第 200 步以後的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# env = gym.make(\"Taxi-v2\") # 舊版本\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# 要調整這兩個數值，以得到最合適的\n",
    "gamma = 0.95 # 因為不是用 deterministic environment 所以 gamma 要比 1 小\n",
    "learning_rate = 0.9 # learning rate 是要實驗才知道哪個數值合適，要在現在的觀察結果與過去的經驗值之間取得平衡\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        # 6. Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]\n",
    "        # 這裡的 Q 值計算，和 rl06_FrozenLakeStochastic.py 不同\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] \\\n",
    "                           + learning_rate * (reward + gamma * torch.max(Q[new_state]))\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(score)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            print(score)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total[200:])), rewards_total[200:], alpha=0.6, color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26afcc",
   "metadata": {},
   "source": [
    "* Stochastic: $Q(s, a) = (1 - \\alpha)Q(s, a) + \\alpha [ r + \\gamma \\times \\max_{a'}Q(s', a')]$\n",
    "* 試試看不同的 learning rate\n",
    "  * 用 learning rate $\\alpha = 0$ (只注重 experience，不管 current observation)\n",
    "```\n",
    "Percentage of episodes finished successfully: -771.102\n",
    "Percentage of episodes finished successfully (last 100 episodes): -769.36\n",
    "Average number of steps: 196.34\n",
    "Average number of steps (last 100 episodes): 194.71\n",
    "```\n",
    "  * 用 learning rate $\\alpha = 1$ (不管過去的 experience，只注重 current observation)\n",
    "```\n",
    "Percentage of episodes finished successfully: -13.093\n",
    "Percentage of episodes finished successfully (last 100 episodes): 7.54\n",
    "Average number of steps: 26.43\n",
    "Average number of steps (last 100 episodes): 13.46\n",
    "```\n",
    "  * 用 learning rate $\\alpha = 0.5$\n",
    "```\n",
    "Percentage of episodes finished successfully: -25.829\n",
    "Percentage of episodes finished successfully (last 100 episodes): 7.87\n",
    "Average number of steps: 34.40\n",
    "Average number of steps (last 100 episodes): 13.13\n",
    "```\n",
    "* 試試看不同的 gamma\n",
    "  * 用 $\\gamma = 0$ (只在乎 current reward)\n",
    "    * 有學習但是學不好，亂 drop off 乘客\n",
    "```\n",
    "Percentage of episodes finished successfully: -194.968\n",
    "Percentage of episodes finished successfully (last 100 episodes): -190.13\n",
    "Average number of steps: 190.55\n",
    "Average number of steps (last 100 episodes): 192.23\n",
    "```\n",
    "  * 用 $\\gamma = 1$ (future reward 也考慮)\n",
    "```\n",
    "Percentage of episodes finished successfully: -19.19\n",
    "Percentage of episodes finished successfully (last 100 episodes): 7.4\n",
    "Average number of steps: 29.08\n",
    "Average number of steps (last 100 episodes): 13.60\n",
    "```\n",
    "* 用 $\\epsilon$ greedy 的方法\n",
    "  * $\\gamma = 0.95$, learning rate $\\alpha = 0.9$, $\\epsilon = 0.1$\n",
    "```\n",
    "Percentage of episodes finished successfully: -24.683\n",
    "Percentage of episodes finished successfully (last 100 episodes): 3.01\n",
    "Average number of steps: 30.00\n",
    "Average number of steps (last 100 episodes): 14.84\n",
    "```\n",
    "  * $\\gamma = 0.95$, learning rate $\\alpha = 0.9$, $\\epsilon = 0$\n",
    "```\n",
    "Percentage of episodes finished successfully: -15.745\n",
    "Percentage of episodes finished successfully (last 100 episodes): 7.85\n",
    "Average number of steps: 28.49\n",
    "Average number of steps (last 100 episodes): 13.15\n",
    "```\n",
    "  * $\\gamma = 0.95$, learning rate $\\alpha = 0.9$, $\\epsilon = 1$\n",
    "    * 沒學任何東西，只是用 random action 的方式\n",
    "```\n",
    "Percentage of episodes finished successfully: -766.486\n",
    "Percentage of episodes finished successfully (last 100 episodes): -776.62\n",
    "Average number of steps: 196.47\n",
    "Average number of steps (last 100 episodes): 198.58\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# env = gym.make(\"Taxi-v2\") # 舊版本\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# 要調整這兩個數值，以得到最合適的\n",
    "gamma = 0.95 # 因為不是用 deterministic environment 所以 gamma 要比 1 小\n",
    "learning_rate = 0 # learning rate 是要實驗才知道哪個數值合適，要在現在的觀察結果與過去的經驗值之間取得平衡\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        # 6. Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]\n",
    "        # 這裡的 Q 值計算，和 rl06_FrozenLakeStochastic.py 不同\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] \\\n",
    "                           + learning_rate * (reward + gamma * torch.max(Q[new_state]))\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(score)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            print(score)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed8f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# env = gym.make(\"Taxi-v2\") # 舊版本\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# 要調整這兩個數值，以得到最合適的\n",
    "gamma = 0.95 # 因為不是用 deterministic environment 所以 gamma 要比 1 小\n",
    "learning_rate = 1 # learning rate 是要實驗才知道哪個數值合適，要在現在的觀察結果與過去的經驗值之間取得平衡\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        # 6. Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]\n",
    "        # 這裡的 Q 值計算，和 rl06_FrozenLakeStochastic.py 不同\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] \\\n",
    "                           + learning_rate * (reward + gamma * torch.max(Q[new_state]))\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(score)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            print(score)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# env = gym.make(\"Taxi-v2\") # 舊版本\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# 要調整這兩個數值，以得到最合適的\n",
    "gamma = 0.95 # 因為不是用 deterministic environment 所以 gamma 要比 1 小\n",
    "learning_rate = 0.5 # learning rate 是要實驗才知道哪個數值合適，要在現在的觀察結果與過去的經驗值之間取得平衡\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        # 6. Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]\n",
    "        # 這裡的 Q 值計算，和 rl06_FrozenLakeStochastic.py 不同\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] \\\n",
    "                           + learning_rate * (reward + gamma * torch.max(Q[new_state]))\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(score)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            print(score)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# env = gym.make(\"Taxi-v2\") # 舊版本\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# 要調整這兩個數值，以得到最合適的\n",
    "gamma = 0 # 因為不是用 deterministic environment 所以 gamma 要比 1 小\n",
    "learning_rate = 0.9 # learning rate 是要實驗才知道哪個數值合適，要在現在的觀察結果與過去的經驗值之間取得平衡\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        # 6. Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]\n",
    "        # 這裡的 Q 值計算，和 rl06_FrozenLakeStochastic.py 不同\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] \\\n",
    "                           + learning_rate * (reward + gamma * torch.max(Q[new_state]))\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(score)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            print(score)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d59c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# env = gym.make(\"Taxi-v2\") # 舊版本\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# 要調整這兩個數值，以得到最合適的\n",
    "gamma = 1 # 因為不是用 deterministic environment 所以 gamma 要比 1 小\n",
    "learning_rate = 0.9 # learning rate 是要實驗才知道哪個數值合適，要在現在的觀察結果與過去的經驗值之間取得平衡\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        # 4. select and perform action a\n",
    "        random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "        action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "        # 因此要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        # 6. Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]\n",
    "        # 這裡的 Q 值計算，和 rl06_FrozenLakeStochastic.py 不同\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] \\\n",
    "                           + learning_rate * (reward + gamma * torch.max(Q[new_state]))\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(score)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            print(score)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43153aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# env = gym.make(\"Taxi-v2\") # 舊版本\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# 要調整這兩個數值，以得到最合適的\n",
    "gamma = 0.95 # 因為不是用 deterministic environment 所以 gamma 要比 1 小\n",
    "learning_rate = 0.9 # learning rate 是要實驗才知道哪個數值合適，要在現在的觀察結果與過去的經驗值之間取得平衡\n",
    "egreedy = 0.1\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        # 4. select and perform action a\n",
    "        if random_for_egreedy > egreedy:\n",
    "            random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "            action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "            # 要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        else:\n",
    "            action = env.action_space.sample()        \n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        # 6. Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]\n",
    "        # 這裡的 Q 值計算，和 rl06_FrozenLakeStochastic.py 不同\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] \\\n",
    "                           + learning_rate * (reward + gamma * torch.max(Q[new_state]))\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(score)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            print(score)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total[200:])), rewards_total[200:], alpha=0.6, color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# env = gym.make(\"Taxi-v2\") # 舊版本\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# 要調整這兩個數值，以得到最合適的\n",
    "gamma = 0.95 # 因為不是用 deterministic environment 所以 gamma 要比 1 小\n",
    "learning_rate = 0.9 # learning rate 是要實驗才知道哪個數值合適，要在現在的觀察結果與過去的經驗值之間取得平衡\n",
    "egreedy = 0\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        # 4. select and perform action a\n",
    "        if random_for_egreedy > egreedy:\n",
    "            random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "            action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "            # 要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        else:\n",
    "            action = env.action_space.sample()        \n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        # 6. Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]\n",
    "        # 這裡的 Q 值計算，和 rl06_FrozenLakeStochastic.py 不同\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] \\\n",
    "                           + learning_rate * (reward + gamma * torch.max(Q[new_state]))\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(score)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            print(score)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total[200:])), rewards_total[200:], alpha=0.6, color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# env = gym.make(\"Taxi-v2\") # 舊版本\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "number_of_states = env.observation_space.n # 用 .n 得到 space 中有幾個數 \n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "# 要調整這兩個數值，以得到最合適的\n",
    "gamma = 0.95 # 因為不是用 deterministic environment 所以 gamma 要比 1 小\n",
    "learning_rate = 0.9 # learning rate 是要實驗才知道哪個數值合適，要在現在的觀察結果與過去的經驗值之間取得平衡\n",
    "egreedy = 1\n",
    "\n",
    "Q = torch.zeros([number_of_states, number_of_actions]) # 1. initial Q[num_state, num_action]\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 2. observe initial state s\n",
    "\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True: # 3. repeat until terminated\n",
    "        step += 1\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        # 4. select and perform action a\n",
    "        if random_for_egreedy > egreedy:\n",
    "            random_values = Q[state] + torch.rand(1, number_of_actions) / 1000\n",
    "            action = torch.max(random_values, 1)[1].tolist()[0]\n",
    "            # 要加入一點很小的 randomness 使得有 exploration 且又不影響到 Q value\n",
    "        else:\n",
    "            action = env.action_space.sample()        \n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) # 等號右邊是 4. select and perform action a 等號左邊是 5. observe reward r and new state s'\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        # 6. Q(s, a) = (1 - alpha)Q(s, a) + alpha[r + gamma * max Q(s', a')]\n",
    "        # 這裡的 Q 值計算，和 rl06_FrozenLakeStochastic.py 不同\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] \\\n",
    "                           + learning_rate * (reward + gamma * torch.max(Q[new_state]))\n",
    "        state = new_state # 7. s = s'\n",
    "        \n",
    "        # time.sleep(0.4) \n",
    "        # env.render() # 把圖案畫出來\n",
    "        \n",
    "        # print(new_state) # 每走了一步之後變成新的狀態\n",
    "        # print(info)\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(score)\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            print(score)\n",
    "            break\n",
    "\n",
    "print(\"Final Q-table:\\n\", Q)\n",
    "\n",
    "print(\"Percentage of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percentage of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Steps / Episode length\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total[200:])), rewards_total[200:], alpha=0.6, color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac4c1f2",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "* [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)\n",
    "* Deep learning: when the hidden layers $\\ge$ 2 can be called as deep learning\n",
    "* Activation functions:\n",
    "  * ReLU: DeepMind\n",
    "  * Tanh: OpenAI\n",
    "  * The activation function is used to determine if our neural network be actived or not\n",
    "* Loss functions:\n",
    "  * MSE\n",
    "  * Huber Loss: DeepMind\n",
    "    * The Huber loss is quadratic for small values and linear for large values.\n",
    "      * To penalize our network class for huge mistakes comparing to MSE.\n",
    "  * The loss function is used to see how good are we doing or how far away from the expected value we are\n",
    "    * The difference between the predicted value and the actual outputs.\n",
    "* Optimizer:\n",
    "  * Adman\n",
    "  * RMSProp\n",
    "  * The optimizer is used to minize our loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866a9a3",
   "metadata": {},
   "source": [
    "### `torch.squeeze()` and `torch.unsqueeze()`\n",
    "* `torch.squeeze()` 把 tensor 中 dim=1 的部分移除\n",
    "  * 如果有指明哪一個 dim 要移除，若該 dim = 1 就只會移除該 dim，若該 dim != 1 就什麼都不做\n",
    "* `torch.unsqueeze()` 在指定的 dim 部分加入 dim=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c53786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0.]],\n",
       "\n",
       "         [[0., 0.]]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "y = torch.zeros(1, 2, 1, 2)\n",
    "y # size=1x2x1x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f9155d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(y) # size=2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236782fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.]],\n",
       "\n",
       "        [[0., 0.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(y, 0) # get rid of the first dim: size=2x1x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d87750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0.]],\n",
       "\n",
       "         [[0., 0.]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(y, 1) # get rid of the 2nd dim, but the 2nd dim != 1, so the size=1x2x1x2 is unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec7bc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 6.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.Tensor([1, 3, 6])\n",
    "z # size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "672009be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 3., 6.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(z, 0) # size=1x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7954907e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [3.],\n",
       "        [6.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(z, 1) # size=3x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586c453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# if GPU is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd5d10",
   "metadata": {},
   "source": [
    "目標是要用 NN 來猜 linear function: $y = W \\cdot x + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189c4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 2\n",
    "b = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df9a4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
       "        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "        90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(100)\n",
    "print(x.size())\n",
    "x # size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2df824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
       "        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "        90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.arange(100)) # pytorch 要求變數要放在 Variable()\n",
    "print(x.size())\n",
    "x # size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "915e00b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "torch.LongTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 1],\n",
       "        [ 2],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [11],\n",
       "        [12],\n",
       "        [13],\n",
       "        [14],\n",
       "        [15],\n",
       "        [16],\n",
       "        [17],\n",
       "        [18],\n",
       "        [19],\n",
       "        [20],\n",
       "        [21],\n",
       "        [22],\n",
       "        [23],\n",
       "        [24],\n",
       "        [25],\n",
       "        [26],\n",
       "        [27],\n",
       "        [28],\n",
       "        [29],\n",
       "        [30],\n",
       "        [31],\n",
       "        [32],\n",
       "        [33],\n",
       "        [34],\n",
       "        [35],\n",
       "        [36],\n",
       "        [37],\n",
       "        [38],\n",
       "        [39],\n",
       "        [40],\n",
       "        [41],\n",
       "        [42],\n",
       "        [43],\n",
       "        [44],\n",
       "        [45],\n",
       "        [46],\n",
       "        [47],\n",
       "        [48],\n",
       "        [49],\n",
       "        [50],\n",
       "        [51],\n",
       "        [52],\n",
       "        [53],\n",
       "        [54],\n",
       "        [55],\n",
       "        [56],\n",
       "        [57],\n",
       "        [58],\n",
       "        [59],\n",
       "        [60],\n",
       "        [61],\n",
       "        [62],\n",
       "        [63],\n",
       "        [64],\n",
       "        [65],\n",
       "        [66],\n",
       "        [67],\n",
       "        [68],\n",
       "        [69],\n",
       "        [70],\n",
       "        [71],\n",
       "        [72],\n",
       "        [73],\n",
       "        [74],\n",
       "        [75],\n",
       "        [76],\n",
       "        [77],\n",
       "        [78],\n",
       "        [79],\n",
       "        [80],\n",
       "        [81],\n",
       "        [82],\n",
       "        [83],\n",
       "        [84],\n",
       "        [85],\n",
       "        [86],\n",
       "        [87],\n",
       "        [88],\n",
       "        [89],\n",
       "        [90],\n",
       "        [91],\n",
       "        [92],\n",
       "        [93],\n",
       "        [94],\n",
       "        [95],\n",
       "        [96],\n",
       "        [97],\n",
       "        [98],\n",
       "        [99]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.arange(100).unsqueeze(1)) # pytorch 要求變數有特定的格式\n",
    "print(x.size())\n",
    "print(x.type())\n",
    "x # size=100x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064c1a6",
   "metadata": {},
   "source": [
    "### rl11_NN_review.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9eefd4e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/04/d8nq6g4s1nb98s7ztbqzxb6m0000gn/T/ipykernel_6960/1855856775.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode %i, loss %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# if GPU is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "# NN 不知道這兩個數值，要藉由學習猜出來\n",
    "W = 2\n",
    "b = 0.2\n",
    "\n",
    "x = Variable(torch.arange(100).unsqueeze(1))\n",
    "\n",
    "y = W * x + b\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.01\n",
    "num_episodes = 1000\n",
    "\n",
    "class NeuralNetwork(nn.Module): # 繼承自 nn.Module 所以可以用 pythorch 提供的 methods\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__() # 要呼叫 parent class\n",
    "        self.linear1 = nn.Linear(1, 1) # 使用 linear model 且 input dim=1, output dim=1\n",
    "        \n",
    "    def forward(self, x): # 收到 input variable x 然後輸出 output variable\n",
    "        output = self.linear1(x)\n",
    "        return output\n",
    "    \n",
    "mynn = NeuralNetwork()\n",
    "\n",
    "if use_cuda:\n",
    "    mynn.cuda() # 如果有 GPU 就用 cuda\n",
    "    \n",
    "loss_func = nn. MSELoss()\n",
    "# loss_func = nn.SmoothL1Loss() # Huber loss\n",
    "\n",
    "optimizer = optim.Adam(params=mynn.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.RMSprop(params=mynn.parameterseters(), lr=learning_rate) # 用 RMSprop 當 loss function\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    predicted_value = mynn(x.type(torch.FloatTensor)) # RuntimeError: expected scalar type Float but found Long\n",
    "    loss = loss_func(predicted_value, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() # 計算 gradient\n",
    "    optimizer.step() # update model parameters\n",
    "    \n",
    "    if i_episode % 50 == 0:\n",
    "        print(\"Episode %i, loss %.4f\" % (i_episode, loss.data[0]))\n",
    "        \n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(x.data.numpy(), y.data.numpy(), alpha=0.6, color=\"green\") # 不能直接放 pytorch tensor 給 matplotlib 所以要加上 .data.numpy()\n",
    "plt.plot(x.data.numpy(), predicted_value.data.numpy(), alpha=0.6, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea1042",
   "metadata": {},
   "source": [
    "### rl11_NN-review-0.4.0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce994fac",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/04/d8nq6g4s1nb98s7ztbqzxb6m0000gn/T/ipykernel_6960/698459088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mpredicted_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmynn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/04/d8nq6g4s1nb98s7ztbqzxb6m0000gn/T/ipykernel_6960/698459088.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Long"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "W = 2\n",
    "b = 0.3\n",
    "\n",
    "x = torch.arange(100).to(device).unsqueeze(1)\n",
    "\n",
    "y = W * x + b\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.01\n",
    "num_episodes = 1000\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear1(x)\n",
    "        return output\n",
    "    \n",
    "mynn = NeuralNetwork().to(device)\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "# loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "optimizer = optim.Adam(params=mynn.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    predicted_value = mynn(x)\n",
    "    \n",
    "    loss = loss_func(predicted_value, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i_episode % 50 == 0:\n",
    "        print(\"Episode %i, loss %.4f\" % (i_episode, loss.item()))\n",
    "        \n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(x.cpu().numpy(), y.cpu().numpy(), alpha=0.6, color=\"green\") # numpy only work on cpu so we have to call cpu()\n",
    "plt.plot(x.cpu().numpy(), predicted_value.detach().cpu().numpy(), alpha=0.6, color=\"blue\")\n",
    "\n",
    "if use_cuda:\n",
    "    plt.savefig(\"graph.png\")\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f138601",
   "metadata": {},
   "source": [
    "### rl12_CartPoleRandomNew.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"CartPole-v1\") # 建立環境，這邊用 gym 中已經設定好的環境\n",
    "\n",
    "# 設定 random seed\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "num_episodes = 1000 # 跑 1000 個 episodes\n",
    "steps_total = [] # 用來儲存每個 episode 走了幾步\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 最初要先 reset 狀態\n",
    "    \n",
    "    step = 0 # 每個 episode 一開始的步數要先歸零\n",
    "    while True:\n",
    "        step += 1\n",
    "        action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        new_state, reward, done, info = env.step(action) # 告訴環境 agent 用了哪個 action\n",
    "\n",
    "#         print(new_state) # 每走了一步之後變成新的狀態\n",
    "#         print(info)\n",
    "\n",
    "#         env.render() # 把圖案畫出來，會畫在新的 window 中，不會畫在 jupyter notebook 中\n",
    "\n",
    "        if done:\n",
    "            steps_total.append(step) # 每個 episode 共走了幾部記錄下來\n",
    "            print(\"Episode finised after %i steps\" % step)\n",
    "            break\n",
    "\n",
    "# 在 CartPole 中每一個 step 就得到一 reward 所以兩者是等價的，因此只需要一張圖就好\n",
    "print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4c622",
   "metadata": {},
   "source": [
    "### rl13_egreedy_tool.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb755d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "num_episodes = 150\n",
    "\n",
    "egreedy = 0.7\n",
    "egreedy_final = 0.1\n",
    "egreedy_decay = 500\n",
    "\n",
    "egreedy_prev = egreedy\n",
    "egreedy_prev_final = egreedy_final\n",
    "egreedy_prev_decay = 0.999\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "egreedy_total = []\n",
    "egreedy_prev_total = []\n",
    "\n",
    "steps_total = 0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        steps_total += 1\n",
    "        epsilon = calculate_epsilon(steps_total)\n",
    "        egreedy_total.append(epsilon)\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        state = new_state\n",
    "        if egreedy_prev > egreedy_prev_final:\n",
    "            egreedy_prev *= egreedy_prev_decay\n",
    "            egreedy_prev_total.append(egreedy_prev)\n",
    "        else:\n",
    "            egreedy_prev_total.append(egreedy_prev)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Egreedy value\")\n",
    "plt.bar(torch.arange(len(egreedy_total)), egreedy_total, alpha=0.6, color=\"blue\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Egreedy 2 value\")\n",
    "plt.bar(torch.arange(len(egreedy_prev_total)), egreedy_prev_total, alpha=0.6, color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80390211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "num_episodes = 150\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.02\n",
    "egreedy_decay = 500\n",
    "\n",
    "egreedy_prev = egreedy\n",
    "egreedy_prev_final = egreedy_final\n",
    "egreedy_prev_decay = 0.999\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "egreedy_total = []\n",
    "egreedy_prev_total = []\n",
    "\n",
    "steps_total = 0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        steps_total += 1\n",
    "        epsilon = calculate_epsilon(steps_total)\n",
    "        egreedy_total.append(epsilon)\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        state = new_state\n",
    "        if egreedy_prev > egreedy_prev_final:\n",
    "            egreedy_prev *= egreedy_prev_decay\n",
    "            egreedy_prev_total.append(egreedy_prev)\n",
    "        else:\n",
    "            egreedy_prev_total.append(egreedy_prev)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Egreedy value\")\n",
    "plt.bar(torch.arange(len(egreedy_total)), egreedy_total, alpha=0.6, color=\"blue\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Egreedy 2 value\")\n",
    "plt.bar(torch.arange(len(egreedy_prev_total)), egreedy_prev_total, alpha=0.6, color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5578e5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf764d",
   "metadata": {},
   "source": [
    "### rl14_CartPole-NN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7483bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.01\n",
    "# num_episodes = 1000\n",
    "# gamma = 0.99\n",
    "num_episodes = 2000\n",
    "gamma = 0.86\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.02\n",
    "egreedy_decay = 500\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, number_of_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear1(x)\n",
    "        return output\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            print(\"Episode finished after %i steps\" % step)\n",
    "            break\n",
    "            \n",
    "print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43623a",
   "metadata": {},
   "source": [
    "### rl15_CartPole-NN-log.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a559301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.01\n",
    "# num_episodes = 1000\n",
    "# gamma = 0.99\n",
    "num_episodes = 2000\n",
    "gamma = 0.86\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.02\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, number_of_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear1(x)\n",
    "        return output\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0):\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(steps_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(steps_total)/len(steps_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284ad6a",
   "metadata": {},
   "source": [
    "### rl16_CartPole-NN-2layer.py\n",
    "* 嘗試用兩層 hidden layers 看看結果是否有變好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d719dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.01\n",
    "num_episodes = 500\n",
    "gamma = 0.99\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.02\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        # self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0):\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(steps_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(steps_total)/len(steps_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361de61",
   "metadata": {},
   "source": [
    "### rl17_CartPole-Challenge.py\n",
    "* 程式碼和 [`rl16_CartPole-NN-2layer.py`](#rl16_CartPole-NN-2layer.py) 一模一樣，只是改變了 parameters 來看對結果有什麼影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74aec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.02\n",
    "num_episodes = 500\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        # self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        \n",
    "        reward = Tensor([reward]).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + gamma * max_new_state_values\n",
    "        \n",
    "        predicted_value = self.nn(state)[action]\n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0):\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(steps_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(steps_total)/len(steps_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e574af42",
   "metadata": {},
   "source": [
    "## Deep learning 有兩個問題\n",
    "* highly correlated data\n",
    "  * data are strongly linked together\n",
    "  * RL 的 sequence of observations 之間會有 correlation\n",
    "* non-stationary distributions\n",
    "  * Non-stationary 表示 not time independent, the value changes over time and they are not constant\n",
    "  * $Q(s, a) = r + \\gamma \\max_{a'}Q(s', a')$\n",
    "    * 用先前的 $Q(s', a')$ 來計算現在的 $Q(s, a)$\n",
    "      * 用下一個可能的 action 來計算 $Q(s', a')$ 的時候，全部可能的 action 都要拿來算 $Q(s', a')$，然後選用結果是最大的 $Q$-value 那個\n",
    "    * 可是用 neural network  來近似 $Q$-value 的時候，target $Q$-value 就會隨著時間而改變\n",
    "      * Neural network 一直在猜可能的 current state 會導致計算出來的 $Q(s', a')$ 每次都改變一點點，所以 $Q(s, a)$ 就跟著改變\n",
    "      \n",
    "      \n",
    "* 引入 neural network 來求 action-value ($Q$) function (是非線性方程) $\\rightarrow$ 導致 RL 會 unstable 或是 diverge\n",
    "  * Observations 之間有 correlations 的話，對 $Q$ 更新一點點就可能使 policy 改變很多而造成 data 的分佈改變\n",
    "  * action-values $Q$ 和 target-values $r + \\gamma\\max_{a'}Q(s', a')$ 之間也有 correlation\n",
    "* 因為有 correlation 就會造成 RL unstablize 所以要想辦法移除 correlation\n",
    "  * 用 experience replay: 藉由 randomizes over data 來消除 correlation\n",
    "  * 用 iterative update 來調整 $Q$ 使其朝 target-value 接近: 週期性的對 $Q$ 更新以降低和 target 之間的 correlation\n",
    "\n",
    "\n",
    "## Experience Reply\n",
    "* Experience replay randomly samples previous transitions, and thereby smooths the training distribution over many past behaviors\n",
    "* experience reply 就是採用 random shuffle input states 的方式來解決前面提到的兩種情況\n",
    "  * highly correlated data\n",
    "  * non-stationary distributions\n",
    "* experience reply 藉由 random sampling 可以移除 correlation in sequence\n",
    "  * 要知道 reply memory 要用多大，要有多少 transition 要存到 buffer 中\n",
    "    * 要是太大則 data 無法 fit into memory 要是太小則只能記錄一小部分的 transition\n",
    "  * 要設定 batch size 所以才知道要把多少 sample pull from buffer\n",
    "  \n",
    "  \n",
    "## Algorithm: DQN with experience reply\n",
    "```\n",
    "initialize replay memory D to capacity N\n",
    "initialize action-value function Q with random weights\n",
    "for episode = 1, M do\n",
    "    initialize sequence s_{1} = {x_1} and preprocessed sequenced \\phi_{1} = \\phi(s_{1})\n",
    "    for t = 1, T do\n",
    "        with probability \\epsilon select a random action a_{t}\n",
    "        otherwise select a_{t} = max_{a} Q*(\\phi(s_{t}, a; \\theta)\n",
    "        execute action a_{t} in emulator and observe reward r_{t} and image x_{t+1}\n",
    "        set s_{t+1} = s_{t}, a_{t}, x_{t+1} and preprocess \\phi_{t+1} = \\phi(s_{t+1})\n",
    "        store transition (\\phi_{t}, a_{t}, r_{t}, \\phi_{t+1}) in D\n",
    "        sample random minibatch of transitions (\\phi_{j}, a_{j}, r_{j}, \\phi_{j+1}) from D\n",
    "        set y_{j} = r_{j} for terminal \\phi_{j+1} or y_{j} = r_{j} + \\gamma max_{a'}Q(\\phi_{j+1}, a';\\theta) for non-terminal \\phi_{j+1}\n",
    "        perform a gradient descent step on (y - Q(\\phi_{j}, a_{j}, \\theta))^2 according equation 3\n",
    "    end for\n",
    "end for\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548baa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 0\n",
    "capacity = 4\n",
    "position = (position + 1) % capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23511e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f556a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = (position + 1) % capacity\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af3d77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = (position + 1) % capacity\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb98f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = (position + 1) % capacity\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610f5926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1, 2, 3], [4, 5, 6]]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14141ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7f80d2fcfb40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c225b8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1, 2, 3],), ([4, 5, 6],)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "593ca27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (3, 6)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*x)) # 有 * 就是把相對應的元素放在同一組 tuple 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6665ce58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.Tensor([1, 0, 0])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddfc8116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a2b22",
   "metadata": {},
   "source": [
    "### rl18_CartPole-ExperienceReplay.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb60eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.02\n",
    "num_episodes = 500\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_memory_size = 50000\n",
    "# batch_size = 32\n",
    "batch_size = 3\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # memory 的大小\n",
    "        self.memory = []\n",
    "        self.position = 0 # 幫忙追蹤放到 memory 中的 entry\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done) # 把所有資訊放到一個 transition 中，然後把 transition 餵給 memory\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # 在 memory 中加入新的 transition\n",
    "        else:\n",
    "            self.memory[self.position] = transition # overwrite 原有的 memory\n",
    "            \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size): # batch_size 是只有多少 entries 要做 sampling\n",
    "        return zip(*random.sample(self.memory, batch_size)) # random.sample(從哪裡取數，要取幾個數)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        # self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # 要取出 barch_size 這麼多個 samples, 那 memory 中必須要有 batch_size 這麼多個 samples 才行\n",
    "        if (len(memory) < batch_size):\n",
    "            return # 當 memory 比 batch_size 小的時候就直接傳回\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        # 要把全部的 data 轉成 tensor\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        # 這時候 reward, action, done 都是 batch 而不是單一的數值\n",
    "        reward = Tensor(reward).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "        \n",
    "        \n",
    "        new_state_values = self.nn(new_state).detach()\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0] # 1 表示從每個 row 中選出最大值, [0] 是取出最大值的數值\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values # 當 done = 1 的時候，1-done=0 所以只會剩下 reward\n",
    "\n",
    "        # print(new_state_values)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        # predicted_value = self.nn(state)[action]\n",
    "        # print(self.nn(state).size())\n",
    "        # print(action.unsqueeze(1).size())\n",
    "        # print(self.nn(state))\n",
    "        # print(action.unsqueeze(1))\n",
    "        # print(self.nn(state).gather(1, action.unsqueeze(1))) # gather 沿著 row 方向照 action 指示的值取出來 state\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print(predicted_value.squeeze(1).size())\n",
    "        # print(target_value.size())\n",
    "        loss = self.loss_func(predicted_value, target_value) # predicted_value 和 target_value 要有相同的 dimension\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(state, action, new_state, reward, done) # 把 experience 收集到 memory 中\n",
    "        qnet_agent.optimize()\n",
    "        # qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0):\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(steps_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(steps_total)/len(steps_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33103f6",
   "metadata": {},
   "source": [
    "## DQN\n",
    "* DQN: Deep $Q$-network\n",
    "\n",
    "\n",
    "## Target Net\n",
    "* Use an iterative update that adjusts the action-values Q toward target values that are only periodically updated, thereby reducing correlations with the target.\n",
    "* $L_{i}(\\theta_{i}) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\Big[ \\Big( r + \\gamma max_{a'}Q(s', a';\\theta_{i}^{-}) - Q(s, a; \\theta_{i})\\Big) \\Big]$\n",
    "  * target value: $r + \\gamma\\max_{a'}Q(s', a'; \\theta_{i}^{-})$\n",
    "    * 要 stablize NN performance 的話，就不要常常改動 target 的數值\n",
    "    * 用 Target net 來更新 $Q(s', a'; \\theta_{i}^{-})$\n",
    "      * 在一定的 number of steps 內要維持 fixed target value\n",
    "  * predicted value: $Q(s, a; \\theta_{i})$\n",
    "    * 用 learning net 來更新 $Q(s, a; \\theta_{i})$\n",
    "  * 用 learning NN 來更新 target net\n",
    "    * 就是更新 weight 的值\n",
    "    * 其實就是每隔一定的 number of steps 把 learning nn 拷貝到 target net\n",
    "\n",
    "\n",
    "* 當我們有 fix data to learn from 和 fix target to compare to 的時候，這樣會更類似 supervise learning\n",
    "\n",
    "* How often we control to update target net?\n",
    "  * 用 hyperparameters 來調\n",
    "    * too high: 沒有新的資訊輸入造成 target 幾乎沒變 $\\Rightarrow$ doesn't learning anything\n",
    "    * too low: target 常常改變 non-stationary\n",
    "  * 建議每隔 10 $\\sim$ 100 或 $\\sim$ 1000 步來更新\n",
    "  * 會對結果有很大的影響，所以要先決定是要快得到結果，還是要得到穩定的結果\n",
    "\n",
    "\n",
    "## Error Clipping\n",
    "* Clip the error term from the update $r + \\gamma \\max_{a'}Q(s', a';\\theta_{i}^{-}) - Q(s, a; \\theta_{i})$ to be between  -1 and 1\n",
    "  * Absolute value loss function $|x|$ 的導數是\n",
    "    * -1, $x \\lt 0$\n",
    "    * 1, $x \\gt 0$\n",
    "* 用 error clipping 把誤差限制在 -1 ~ +1 之間，可以增進 stability\n",
    "* python implemention\n",
    "```python\n",
    "loss.backward()\n",
    "\n",
    "for param in self.agent.parameters():\n",
    "    param.grad.data.clamp_(-1, 1)\n",
    "    \n",
    "self.optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18810549",
   "metadata": {},
   "source": [
    "### rl19_CartPole-targetnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.01\n",
    "num_episodes = 500\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_memory_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 500 # 每 500 steps 更新一次 target network\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "clip_error = False\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # memory 的大小\n",
    "        self.memory = []\n",
    "        self.position = 0 # 幫忙追蹤放到 memory 中的 entry\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done) # 把所有資訊放到一個 transition 中，然後把 transition 餵給 memory\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # 在 memory 中加入新的 transition\n",
    "        else:\n",
    "            self.memory[self.position] = transition # overwrite 原有的 memory\n",
    "            \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size): # batch_size 是只有多少 entries 要做 sampling\n",
    "        return zip(*random.sample(self.memory, batch_size)) # random.sample(從哪裡取數，要取幾個數)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        # self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0 # 用來追蹤什麼時候需要更新 target net\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # 要取出 barch_size 這麼多個 samples, 那 memory 中必須要有 batch_size 這麼多個 samples 才行\n",
    "        if (len(memory) < batch_size):\n",
    "            return # 當 memory 比 batch_size 小的時候就直接傳回\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        # 要把全部的 data 轉成 tensor\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        # 這時候 reward, action, done 都是 batch 而不是單一的數值\n",
    "        reward = Tensor(reward).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "        \n",
    "        \n",
    "        # new_state_values = self.nn(new_state).detach()\n",
    "        new_state_values = self.target_nn(new_state).detach() # 改成用 target network 來計算 new state\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0] # 1 表示從每個 row 中選出最大值, [0] 是取出最大值的數值\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values # 當 done = 1 的時候，1-done=0 所以只會剩下 reward\n",
    "\n",
    "        # print(new_state_values)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        # predicted_value = self.nn(state)[action]\n",
    "        # print(self.nn(state).size())\n",
    "        # print(action.unsqueeze(1).size())\n",
    "        # print(self.nn(state))\n",
    "        # print(action.unsqueeze(1))\n",
    "        # print(self.nn(state).gather(1, action.unsqueeze(1))) # gather 沿著 row 方向照 action 指示的值取出來 state\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print(predicted_value.squeeze(1).size())\n",
    "        # print(target_value.size())\n",
    "        loss = self.loss_func(predicted_value, target_value) # predicted_value 和 target_value 要有相同的 dimension\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1\n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(state, action, new_state, reward, done) # 把 experience 收集到 memory 中\n",
    "        qnet_agent.optimize()\n",
    "        # qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0):\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(steps_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(steps_total)/len(steps_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b377c",
   "metadata": {},
   "source": [
    "* 調整參數看結果有沒有更好\n",
    "  * 第一組參數是能得到 stable 的結果\n",
    "  * 第二組參數是 solve as quick as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb4200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.001\n",
    "num_episodes = 500\n",
    "gamma = 0.999\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_memory_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 500 # 每 500 steps 更新一次 target network\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "clip_error = True\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # memory 的大小\n",
    "        self.memory = []\n",
    "        self.position = 0 # 幫忙追蹤放到 memory 中的 entry\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done) # 把所有資訊放到一個 transition 中，然後把 transition 餵給 memory\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # 在 memory 中加入新的 transition\n",
    "        else:\n",
    "            self.memory[self.position] = transition # overwrite 原有的 memory\n",
    "            \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size): # batch_size 是只有多少 entries 要做 sampling\n",
    "        return zip(*random.sample(self.memory, batch_size)) # random.sample(從哪裡取數，要取幾個數)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        # self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0 # 用來追蹤什麼時候需要更新 target net\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # 要取出 barch_size 這麼多個 samples, 那 memory 中必須要有 batch_size 這麼多個 samples 才行\n",
    "        if (len(memory) < batch_size):\n",
    "            return # 當 memory 比 batch_size 小的時候就直接傳回\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        # 要把全部的 data 轉成 tensor\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        # 這時候 reward, action, done 都是 batch 而不是單一的數值\n",
    "        reward = Tensor(reward).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "        \n",
    "        \n",
    "        # new_state_values = self.nn(new_state).detach()\n",
    "        new_state_values = self.target_nn(new_state).detach() # 改成用 target network 來計算 new state\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0] # 1 表示從每個 row 中選出最大值, [0] 是取出最大值的數值\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values # 當 done = 1 的時候，1-done=0 所以只會剩下 reward\n",
    "\n",
    "        # print(new_state_values)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        # predicted_value = self.nn(state)[action]\n",
    "        # print(self.nn(state).size())\n",
    "        # print(action.unsqueeze(1).size())\n",
    "        # print(self.nn(state))\n",
    "        # print(action.unsqueeze(1))\n",
    "        # print(self.nn(state).gather(1, action.unsqueeze(1))) # gather 沿著 row 方向照 action 指示的值取出來 state\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print(predicted_value.squeeze(1).size())\n",
    "        # print(target_value.size())\n",
    "        loss = self.loss_func(predicted_value, target_value) # predicted_value 和 target_value 要有相同的 dimension\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1\n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(state, action, new_state, reward, done) # 把 experience 收集到 memory 中\n",
    "        qnet_agent.optimize()\n",
    "        # qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0):\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(steps_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(steps_total)/len(steps_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.01\n",
    "num_episodes = 500\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_memory_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 100 # 每 500 steps 更新一次 target network\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "clip_error = False\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # memory 的大小\n",
    "        self.memory = []\n",
    "        self.position = 0 # 幫忙追蹤放到 memory 中的 entry\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done) # 把所有資訊放到一個 transition 中，然後把 transition 餵給 memory\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # 在 memory 中加入新的 transition\n",
    "        else:\n",
    "            self.memory[self.position] = transition # overwrite 原有的 memory\n",
    "            \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size): # batch_size 是只有多少 entries 要做 sampling\n",
    "        return zip(*random.sample(self.memory, batch_size)) # random.sample(從哪裡取數，要取幾個數)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        # self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0 # 用來追蹤什麼時候需要更新 target net\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # 要取出 barch_size 這麼多個 samples, 那 memory 中必須要有 batch_size 這麼多個 samples 才行\n",
    "        if (len(memory) < batch_size):\n",
    "            return # 當 memory 比 batch_size 小的時候就直接傳回\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        # 要把全部的 data 轉成 tensor\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        # 這時候 reward, action, done 都是 batch 而不是單一的數值\n",
    "        reward = Tensor(reward).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "        \n",
    "        \n",
    "        # new_state_values = self.nn(new_state).detach()\n",
    "        new_state_values = self.target_nn(new_state).detach() # 改成用 target network 來計算 new state\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0] # 1 表示從每個 row 中選出最大值, [0] 是取出最大值的數值\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values # 當 done = 1 的時候，1-done=0 所以只會剩下 reward\n",
    "\n",
    "        # print(new_state_values)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        # predicted_value = self.nn(state)[action]\n",
    "        # print(self.nn(state).size())\n",
    "        # print(action.unsqueeze(1).size())\n",
    "        # print(self.nn(state))\n",
    "        # print(action.unsqueeze(1))\n",
    "        # print(self.nn(state).gather(1, action.unsqueeze(1))) # gather 沿著 row 方向照 action 指示的值取出來 state\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print(predicted_value.squeeze(1).size())\n",
    "        # print(target_value.size())\n",
    "        loss = self.loss_func(predicted_value, target_value) # predicted_value 和 target_value 要有相同的 dimension\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1\n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(state, action, new_state, reward, done) # 把 experience 收集到 memory 中\n",
    "        qnet_agent.optimize()\n",
    "        # qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0):\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(steps_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(steps_total)/len(steps_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ab69f",
   "metadata": {},
   "source": [
    "## Double DQN\n",
    "\n",
    "* 某些情況下 $Q$-learning 會對 action-value 造成 overestimate\n",
    "  * 因為每次都選最大 $Q$-value 的 action，但是這個可能不是最好的 action\n",
    "  \n",
    "* $Y_{t}^{DoubleDQN} = R_{t+1} + \\gamma Q(S_{t+1}, \\argmax_{a} Q(S_{t+1}, a; \\theta_{t}), \\theta_{t}^{-})$\n",
    "  * 用兩個 NN 來求 $Q$-value\n",
    "    * 一個 NN (learning net) 用來找出 best action: $\\argmax_{a} Q(S_{t+1}, a; \\theta_{t})$\n",
    "    * 另一個 NN (target net) 用來求出 $Q$-value: $Q(S_{t+1}, a_{best}, \\theta_{t}^{-})$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54387901",
   "metadata": {},
   "source": [
    "### rl20_CartPole-DoubleDQN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.01\n",
    "num_episodes = 500\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_memory_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 100 # 每 500 steps 更新一次 target network\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "clip_error = False\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # memory 的大小\n",
    "        self.memory = []\n",
    "        self.position = 0 # 幫忙追蹤放到 memory 中的 entry\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done) # 把所有資訊放到一個 transition 中，然後把 transition 餵給 memory\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # 在 memory 中加入新的 transition\n",
    "        else:\n",
    "            self.memory[self.position] = transition # overwrite 原有的 memory\n",
    "            \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size): # batch_size 是只有多少 entries 要做 sampling\n",
    "        return zip(*random.sample(self.memory, batch_size)) # random.sample(從哪裡取數，要取幾個數)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        # self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0 # 用來追蹤什麼時候需要更新 target net\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # 要取出 barch_size 這麼多個 samples, 那 memory 中必須要有 batch_size 這麼多個 samples 才行\n",
    "        if (len(memory) < batch_size):\n",
    "            return # 當 memory 比 batch_size 小的時候就直接傳回\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        # 要把全部的 data 轉成 tensor\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        # 這時候 reward, action, done 都是 batch 而不是單一的數值\n",
    "        reward = Tensor(reward).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "        \n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach() # 用 learning NN 找出最佳的 action\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1] # 用 [1] 取出 index\n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach() # 用 target NN 來求出 Q\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            # new_state_values = self.nn(new_state).detach()\n",
    "            new_state_values = self.target_nn(new_state).detach() # 改成用 target network 來計算 new state\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0] # 1 表示從每個 row 中選出最大值, [0] 是取出最大值的數值\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values # 當 done = 1 的時候，1-done=0 所以只會剩下 reward\n",
    "\n",
    "        # print(new_state_values)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        # predicted_value = self.nn(state)[action]\n",
    "        # print(self.nn(state).size())\n",
    "        # print(action.unsqueeze(1).size())\n",
    "        # print(self.nn(state))\n",
    "        # print(action.unsqueeze(1))\n",
    "        # print(self.nn(state).gather(1, action.unsqueeze(1))) # gather 沿著 row 方向照 action 指示的值取出來 state\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print(predicted_value.squeeze(1).size())\n",
    "        # print(target_value.size())\n",
    "        loss = self.loss_func(predicted_value, target_value) # predicted_value 和 target_value 要有相同的 dimension\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1\n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(state, action, new_state, reward, done) # 把 experience 收集到 memory 中\n",
    "        qnet_agent.optimize()\n",
    "        # qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0):\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(steps_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(steps_total)/len(steps_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.001\n",
    "num_episodes = 500\n",
    "gamma = 0.9999\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_memory_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 500 # 每 500 steps 更新一次 target network\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "clip_error = False\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # memory 的大小\n",
    "        self.memory = []\n",
    "        self.position = 0 # 幫忙追蹤放到 memory 中的 entry\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done) # 把所有資訊放到一個 transition 中，然後把 transition 餵給 memory\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # 在 memory 中加入新的 transition\n",
    "        else:\n",
    "            self.memory[self.position] = transition # overwrite 原有的 memory\n",
    "            \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size): # batch_size 是只有多少 entries 要做 sampling\n",
    "        return zip(*random.sample(self.memory, batch_size)) # random.sample(從哪裡取數，要取幾個數)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        # self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0 # 用來追蹤什麼時候需要更新 target net\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # 要取出 barch_size 這麼多個 samples, 那 memory 中必須要有 batch_size 這麼多個 samples 才行\n",
    "        if (len(memory) < batch_size):\n",
    "            return # 當 memory 比 batch_size 小的時候就直接傳回\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        # 要把全部的 data 轉成 tensor\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        # 這時候 reward, action, done 都是 batch 而不是單一的數值\n",
    "        reward = Tensor(reward).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "        \n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach() # 用 learning NN\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1] # 用 [1] 取出 index\n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            # new_state_values = self.nn(new_state).detach()\n",
    "            new_state_values = self.target_nn(new_state).detach() # 改成用 target network 來計算 new state\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0] # 1 表示從每個 row 中選出最大值, [0] 是取出最大值的數值\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values # 當 done = 1 的時候，1-done=0 所以只會剩下 reward\n",
    "\n",
    "        # print(new_state_values)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        # predicted_value = self.nn(state)[action]\n",
    "        # print(self.nn(state).size())\n",
    "        # print(action.unsqueeze(1).size())\n",
    "        # print(self.nn(state))\n",
    "        # print(action.unsqueeze(1))\n",
    "        # print(self.nn(state).gather(1, action.unsqueeze(1))) # gather 沿著 row 方向照 action 指示的值取出來 state\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print(predicted_value.squeeze(1).size())\n",
    "        # print(target_value.size())\n",
    "        loss = self.loss_func(predicted_value, target_value) # predicted_value 和 target_value 要有相同的 dimension\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1\n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(state, action, new_state, reward, done) # 把 experience 收集到 memory 中\n",
    "        qnet_agent.optimize()\n",
    "        # qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0):\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(steps_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(steps_total)/len(steps_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb707f3",
   "metadata": {},
   "source": [
    "## Dueling DQN\n",
    "\n",
    "* 只是做些簡單的修改就有可能大大地改變 efficiency & 結果的 stability\n",
    "* Dueling DQN\n",
    "  * For model free RL\n",
    "  * 有兩個 estimators:\n",
    "    * 一個用來計算 state value function\n",
    "    * 一個用來計算 state dependent action advantage function\n",
    "      * 有些時候某些 action 就比較重要，有些時候 action 就不重要，用哪個都無所謂。\n",
    "* Advantage function: $A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)$\n",
    "  * 就是把 $Q$-value 拆成 $V$ value 和 $A$ value 兩部份分開計算，最後再把兩者結合計算 $Q$-value\n",
    "    * $V$: how good it is to be in a defined state\n",
    "    * $A$: 告訴我們 importance of each action\n",
    "* $Q$-function: $Q(s, a; \\theta, \\alpha, \\beta) = V(s;\\theta, \\beta) + \\Big(A(s, a;\\theta, \\alpha) - \\frac{1}{|\\mathcal{A}|}\\sum_{a'}A(s, a';\\theta, \\alpha)\\Big)$\n",
    "  * $\\frac{1}{|\\mathcal{A}|}\\sum_{a'}A(s, a';\\theta, \\alpha)\\Big)$: mean value of advantage\n",
    "    * $|\\mathcal{A}|$: Number of samples\n",
    "* Dueling DQN 對 actions 越多的環境就越有效\n",
    "* Dueling DQN 只需要把一層 hidden layer 改成兩個部分，一個用來算 $V$-value 另一個用來算 advantage\n",
    "  * value 和 advantage 使用的輸入都是前一層的輸出\n",
    "  * value 只有一個 output\n",
    "  * Advangate 有 number of actions 個 outputs\n",
    "  * 要把 value 和 advantage 的輸出做 fully connection 然後變成輸出\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54860ec8",
   "metadata": {},
   "source": [
    "### rl20_CartPole-DuelingDQN.py\n",
    "* 跑完的結果\n",
    "```\n",
    "Average reward: 128.41\n",
    "Average reward (last 100 episodes): 195.77\n",
    "Solved after 316 episodes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.001\n",
    "num_episodes = 500\n",
    "gamma = 0.9999\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_memory_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 500 # 每 500 steps 更新一次 target network\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "clip_error = False\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # memory 的大小\n",
    "        self.memory = []\n",
    "        self.position = 0 # 幫忙追蹤放到 memory 中的 entry\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done) # 把所有資訊放到一個 transition 中，然後把 transition 餵給 memory\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # 在 memory 中加入新的 transition\n",
    "        else:\n",
    "            self.memory[self.position] = transition # overwrite 原有的 memory\n",
    "            \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size): # batch_size 是只有多少 entries 要做 sampling\n",
    "        return zip(*random.sample(self.memory, batch_size)) # random.sample(從哪裡取數，要取幾個數)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        # self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        # 第二層改為 value 和 advantage\n",
    "        self.advantage = nn.Linear(hidden_layer, number_of_output) # advantage 有 number of actions 個輸出\n",
    "        self.value = nn.Linear(hidden_layer, 1) # value 只有一個輸出\n",
    "        self.activation = nn.Tanh()\n",
    "        # self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        # output2 = self.linear2(output1)\n",
    "        output_advantage = self.advantage(output1,)\n",
    "        output_value = self.value(output1,)\n",
    "        \n",
    "        # 要把 value 和 advantage 的輸出結合起來\n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "        \n",
    "        return output_final\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0 # 用來追蹤什麼時候需要更新 target net\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # 要取出 barch_size 這麼多個 samples, 那 memory 中必須要有 batch_size 這麼多個 samples 才行\n",
    "        if (len(memory) < batch_size):\n",
    "            return # 當 memory 比 batch_size 小的時候就直接傳回\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        # 要把全部的 data 轉成 tensor\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        # 這時候 reward, action, done 都是 batch 而不是單一的數值\n",
    "        reward = Tensor(reward).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "        \n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach() # 用 learning NN\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1] # 用 [1] 取出 index\n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            # new_state_values = self.nn(new_state).detach()\n",
    "            new_state_values = self.target_nn(new_state).detach() # 改成用 target network 來計算 new state\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0] # 1 表示從每個 row 中選出最大值, [0] 是取出最大值的數值\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values # 當 done = 1 的時候，1-done=0 所以只會剩下 reward\n",
    "\n",
    "        # print(new_state_values)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        # predicted_value = self.nn(state)[action]\n",
    "        # print(self.nn(state).size())\n",
    "        # print(action.unsqueeze(1).size())\n",
    "        # print(self.nn(state))\n",
    "        # print(action.unsqueeze(1))\n",
    "        # print(self.nn(state).gather(1, action.unsqueeze(1))) # gather 沿著 row 方向照 action 指示的值取出來 state\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print(predicted_value.squeeze(1).size())\n",
    "        # print(target_value.size())\n",
    "        loss = self.loss_func(predicted_value, target_value) # predicted_value 和 target_value 要有相同的 dimension\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1\n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(state, action, new_state, reward, done) # 把 experience 收集到 memory 中\n",
    "        qnet_agent.optimize()\n",
    "        # qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0):\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(steps_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(steps_total)/len(steps_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cfb58d",
   "metadata": {},
   "source": [
    "* Have a stable result\n",
    "  * 目標是 last 100 episodes 沒有損失任何的點數\n",
    "    * 就是說最後的 100 episodes 的 rewards 都是 200\n",
    "  * 把 update_target_frequency 改成 1500\n",
    "* 跑完的結果\n",
    "```\n",
    "Average reward: 149.26\n",
    "Average reward (last 100 episodes): 198.26\n",
    "Solved after 253 episodes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.001\n",
    "num_episodes = 500\n",
    "gamma = 0.9999\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_memory_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 1500 # 每 500 steps 更新一次 target network\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "clip_error = False\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # memory 的大小\n",
    "        self.memory = []\n",
    "        self.position = 0 # 幫忙追蹤放到 memory 中的 entry\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done) # 把所有資訊放到一個 transition 中，然後把 transition 餵給 memory\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # 在 memory 中加入新的 transition\n",
    "        else:\n",
    "            self.memory[self.position] = transition # overwrite 原有的 memory\n",
    "            \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size): # batch_size 是只有多少 entries 要做 sampling\n",
    "        return zip(*random.sample(self.memory, batch_size)) # random.sample(從哪裡取數，要取幾個數)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        # self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        # 第二層改為 value 和 advantage\n",
    "        self.advantage = nn.Linear(hidden_layer, number_of_output) # advantage 有 number of actions 個輸出\n",
    "        self.value = nn.Linear(hidden_layer, 1) # value 只有一個輸出\n",
    "        self.activation = nn.Tanh()\n",
    "        # self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        # output2 = self.linear2(output1)\n",
    "        output_advantage = self.advantage(output1,)\n",
    "        output_value = self.value(output1,)\n",
    "        \n",
    "        # 要把 value 和 advantage 的輸出結合起來\n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "        \n",
    "        return output_final\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0 # 用來追蹤什麼時候需要更新 target net\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # 要取出 barch_size 這麼多個 samples, 那 memory 中必須要有 batch_size 這麼多個 samples 才行\n",
    "        if (len(memory) < batch_size):\n",
    "            return # 當 memory 比 batch_size 小的時候就直接傳回\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        # 要把全部的 data 轉成 tensor\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        # 這時候 reward, action, done 都是 batch 而不是單一的數值\n",
    "        reward = Tensor(reward).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "        \n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach() # 用 learning NN\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1] # 用 [1] 取出 index\n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            # new_state_values = self.nn(new_state).detach()\n",
    "            new_state_values = self.target_nn(new_state).detach() # 改成用 target network 來計算 new state\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0] # 1 表示從每個 row 中選出最大值, [0] 是取出最大值的數值\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values # 當 done = 1 的時候，1-done=0 所以只會剩下 reward\n",
    "\n",
    "        # print(new_state_values)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        # predicted_value = self.nn(state)[action]\n",
    "        # print(self.nn(state).size())\n",
    "        # print(action.unsqueeze(1).size())\n",
    "        # print(self.nn(state))\n",
    "        # print(action.unsqueeze(1))\n",
    "        # print(self.nn(state).gather(1, action.unsqueeze(1))) # gather 沿著 row 方向照 action 指示的值取出來 state\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print(predicted_value.squeeze(1).size())\n",
    "        # print(target_value.size())\n",
    "        loss = self.loss_func(predicted_value, target_value) # predicted_value 和 target_value 要有相同的 dimension\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1\n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(state, action, new_state, reward, done) # 把 experience 收集到 memory 中\n",
    "        qnet_agent.optimize()\n",
    "        # qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0):\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(steps_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(steps_total)/len(steps_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4c544",
   "metadata": {},
   "source": [
    "* Solve the problem as soon as possible\n",
    "  * 目標是要 beat DoubleDQN 的結果\n",
    "  * 把 gamma 改成 1，把 update_target_frequency 設成 50\n",
    "  * 把 value 和 advantage 各多加一層，所以整個模型裡有三層\n",
    "* 跑完的結果\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.001\n",
    "num_episodes = 500\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_memory_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 50 # 每 500 steps 更新一次 target network\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "egreedy = 1\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "clip_error = False\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # memory 的大小\n",
    "        self.memory = []\n",
    "        self.position = 0 # 幫忙追蹤放到 memory 中的 entry\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done) # 把所有資訊放到一個 transition 中，然後把 transition 餵給 memory\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # 在 memory 中加入新的 transition\n",
    "        else:\n",
    "            self.memory[self.position] = transition # overwrite 原有的 memory\n",
    "            \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size): # batch_size 是只有多少 entries 要做 sampling\n",
    "        return zip(*random.sample(self.memory, batch_size)) # random.sample(從哪裡取數，要取幾個數)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        # self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        # 第二層改為 value 和 advantage\n",
    "        self.advantage = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.advantage2 = nn.Linear(hidden_layer, number_of_output) # advantage 有 number of actions 個輸出\n",
    "        \n",
    "        self.value = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.value2 = nn.Linear(hidden_layer, 1) # value 只有一個輸出\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        # self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        # output2 = self.linear2(output1)\n",
    "        output_advantage = self.advantage(output1)\n",
    "        output_advantage = self.activation(output_advantage)\n",
    "        output_advantage = self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value = self.value(output1)\n",
    "        output_value = self.activation(output_value)\n",
    "        output_value = self.value2(output_value)\n",
    "        \n",
    "        # 要把 value 和 advantage 的輸出結合起來\n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "        \n",
    "        return output_final\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0 # 用來追蹤什麼時候需要更新 target net\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # 要取出 barch_size 這麼多個 samples, 那 memory 中必須要有 batch_size 這麼多個 samples 才行\n",
    "        if (len(memory) < batch_size):\n",
    "            return # 當 memory 比 batch_size 小的時候就直接傳回\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        # 要把全部的 data 轉成 tensor\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        # 這時候 reward, action, done 都是 batch 而不是單一的數值\n",
    "        reward = Tensor(reward).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "        \n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach() # 用 learning NN\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1] # 用 [1] 取出 index\n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            # new_state_values = self.nn(new_state).detach()\n",
    "            new_state_values = self.target_nn(new_state).detach() # 改成用 target network 來計算 new state\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0] # 1 表示從每個 row 中選出最大值, [0] 是取出最大值的數值\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values # 當 done = 1 的時候，1-done=0 所以只會剩下 reward\n",
    "\n",
    "        # print(new_state_values)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        # predicted_value = self.nn(state)[action]\n",
    "        # print(self.nn(state).size())\n",
    "        # print(action.unsqueeze(1).size())\n",
    "        # print(self.nn(state))\n",
    "        # print(action.unsqueeze(1))\n",
    "        # print(self.nn(state).gather(1, action.unsqueeze(1))) # gather 沿著 row 方向照 action 指示的值取出來 state\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print(predicted_value.squeeze(1).size())\n",
    "        # print(target_value.size())\n",
    "        loss = self.loss_func(predicted_value, target_value) # predicted_value 和 target_value 要有相同的 dimension\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1\n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(state, action, new_state, reward, done) # 把 experience 收集到 memory 中\n",
    "        qnet_agent.optimize()\n",
    "        # qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0):\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(steps_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(steps_total)/len(steps_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0032be8",
   "metadata": {},
   "source": [
    "Gym environment 位在"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98fa6f3",
   "metadata": {},
   "source": [
    "## Stanford CS231 course CNN\n",
    "http://cs231n.stanford.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477deda",
   "metadata": {},
   "source": [
    "### rl23_PongRandom.py\n",
    "\n",
    "* 修改 rl12_CartPoleRandomNew.py\n",
    "* 使用 [atari_wrappers.py](https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc674e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from atari_wrappers import make_atari, wrap_deepmind\n",
    "\n",
    "# env = gym.make(\"CartPole-v1\") # 建立環境，這邊用 gym 中已經設定好的環境\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env)\n",
    "\n",
    "# 設定 random seed\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "num_episodes = 5 # 跑 1000 個 episodes\n",
    "rewards_total = [] # 用來儲存每個 episode 走了幾步\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset() # 最初要先 reset 狀態\n",
    "    \n",
    "    score = 0 # 每個 episode 一開始的步數要先歸零\n",
    "    while True:\n",
    "#         step += 1\n",
    "        action = env.action_space.sample() # 從 action space 中隨機選擇 action\n",
    "        new_state, reward, done, info = env.step(action) # 告訴環境 agent 用了哪個 action\n",
    "        score += reward\n",
    "\n",
    "#         print(new_state) # 每走了一步之後變成新的狀態\n",
    "#         print(info)\n",
    "\n",
    "#         env.render() # 把圖案畫出來，會畫在新的 window 中，不會畫在 jupyter notebook 中\n",
    "\n",
    "        if done:\n",
    "            rewards_total.append(score) # 每個 episode 共走了幾部記錄下來\n",
    "            print(\"Episode finised. Rewards: %i\" % score)\n",
    "            break\n",
    "\n",
    "# 在 CartPole 中每一個 step 就得到一 reward 所以兩者是等價的，因此只需要一張圖就好\n",
    "print(\"Average reward: %.2f\" % (sum(rewards_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(rewards_total[-100:])/100))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.plot(rewards_total, alpha=0.6, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f5358",
   "metadata": {},
   "source": [
    "#### Save and load model\n",
    "* https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "* https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3e365",
   "metadata": {},
   "source": [
    "## Flatten tensor\n",
    "\n",
    "[[0, 1], [2, 3]] $\\rightarrow$ [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f92530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 7, 64])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "y = torch.zeros(1, 7, 7, 64)\n",
    "y.shape # 這個是 4-dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef06e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y.view(y.size(0), -1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd1fae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape # 變成 2-dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce7fb2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3136"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7*7*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7acfc9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 84, 84])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randn(1, 84, 84)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4)(y)\n",
    "# Conv2d 預計輸入是 4-dim tensor 但是 y 是 3-dim 所以會有 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2208a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(1, 1, 84, 84)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23821e3a",
   "metadata": {},
   "source": [
    "### rl24_PongVideoOutput.py\n",
    "\n",
    "* Copy rl22_CartPole-Dueling-Tune-Fast.py\n",
    "* DeepMind's CNN\n",
    "\n",
    "|Layer|Input|Filter Size|Stride|# of filters|Activation|Output|\n",
    "|:---|:---|:---|:---|:---|:---|:---|\n",
    "|conv1|84x84x4|8|4|32|ReLU|20x20x32|\n",
    "|conv2|20x20x32|4|2|64|ReLU|9x9x64|\n",
    "|conv3|9x9x64|3|1|64|ReLU|7x7x64|\n",
    "|fc1|7x7x64|||512|ReLU|512|\n",
    "|fc2|512||||Linear|# of actions|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from atari_wrappers import make_atari, wrap_deepmind\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "# env = gym.make(\"CartPole-v0\")\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env)\n",
    "\n",
    "# 把結果存成 video\n",
    "directory = \"./PongVideos/\"\n",
    "env = gym.wrappers.Monitor(env, directory, video_callable=lambda episode_id: episode_id % 20 == 0)\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.001\n",
    "num_episodes = 500\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 512\n",
    "\n",
    "replay_memory_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 50 # 每 500 steps 更新一次 target network\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "egreedy = 1\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "clip_error = False\n",
    "normalize_image = True\n",
    "\n",
    "file2save = \"pong_save.pth\"\n",
    "save_model_frequency = 10000 # 每 1 萬個 frame 就存檔一次\n",
    "resume_previous_training = False\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "def load_model():\n",
    "    return torch.load(file2save)\n",
    "\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), file2save)\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    frame = frame.transpose((2, 0, 1))\n",
    "    frame = torch.from_numpy(frame)\n",
    "    frame = frame.to(device, dtype=torch.float32)\n",
    "    frame = frame.unsqueeze(1)\n",
    "    return frame\n",
    "    \n",
    "def plot_results():\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.plot(rewards_total, alpha=0.6, color=\"red\")\n",
    "#     plt.show()\n",
    "    plt.savefig(\"Pong-results.png\")\n",
    "    plt.close()\n",
    "    \n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # memory 的大小\n",
    "        self.memory = []\n",
    "        self.position = 0 # 幫忙追蹤放到 memory 中的 entry\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done) # 把所有資訊放到一個 transition 中，然後把 transition 餵給 memory\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # 在 memory 中加入新的 transition\n",
    "        else:\n",
    "            self.memory[self.position] = transition # overwrite 原有的 memory\n",
    "            \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size): # batch_size 是只有多少 entries 要做 sampling\n",
    "        return zip(*random.sample(self.memory, batch_size)) # random.sample(從哪裡取數，要取幾個數)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "#         self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        # self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        # 改用 CNN: 有三層 Conv layers 和兩層 FC\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernal_size=8, stride=4) # input 是 1 frames\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernal_size=4, stride=2) # input 是前一級的 output\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernal_size=3, stride=1)\n",
    "        \n",
    "        # 第二層改為 value 和 advantage\n",
    "        self.advantage1 = nn.Linear(7*7*64, hidden_layer)\n",
    "        self.advantage2 = nn.Linear(hidden_layer, number_of_output) # advantage 有 number of actions 個輸出\n",
    "        \n",
    "        self.value1 = nn.Linear(7*7*64, hidden_layer)\n",
    "        self.value2 = nn.Linear(hidden_layer, 1) # value 只有一個輸出\n",
    "        \n",
    "#         self.activation = nn.Tanh()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if normalize_image:\n",
    "            x = x / 255\n",
    "#         output = self.linear1(x)\n",
    "#         output1 = self.activation(output1)\n",
    "        output_conf = self.conv1(x)\n",
    "        output_conf = self.activation(output_conf)\n",
    "        output_conf = self.conv2(output_conf)\n",
    "        output_conf = self.activation(output_conf)\n",
    "        output_conf = self.conv3(output_conf)\n",
    "        output_conf = self.activation(output_conf)\n",
    "        \n",
    "        # flatten: 把 conv 的輸出轉成 2d tensor\n",
    "        output_conf = output_conf.view(output_conf.size(0), -1)\n",
    "        \n",
    "        # output2 = self.linear2(output1)\n",
    "        output_advantage = self.advantage1(output_conf)\n",
    "        output_advantage = self.activation(output_advantage)\n",
    "        output_advantage = self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value = self.value1(output_conf)\n",
    "        output_value = self.activation(output_value)\n",
    "        output_value = self.value2(output_value)\n",
    "        \n",
    "        # 要把 value 和 advantage 的輸出結合起來\n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "        \n",
    "        return output_final\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "#         self.update_target_counter = 0 # 用來追蹤什麼時候需要更新 target net\n",
    "        self.number_of_frames = 0\n",
    "        \n",
    "        if resume_previous_training and os.path.exists(file2save):\n",
    "            print(\"Loading previously saved model ...\")\n",
    "            self.nn.load_state_dict(load_model())\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = preprocess_frame(state)\n",
    "#                 state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "#                 print(\"\\n\\n\\nACTION NN\")\n",
    "#                 print(action_from_nn)\n",
    "                action = torch.max(action_from_nn, 1)[1] # 第二個參數從 0 改成 1\n",
    "#                 print(\"\\n\\nACTION\")\n",
    "#                 print(action)\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # 要取出 barch_size 這麼多個 samples, 那 memory 中必須要有 batch_size 這麼多個 samples 才行\n",
    "        if (len(memory) < batch_size):\n",
    "            return # 當 memory 比 batch_size 小的時候就直接傳回\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "#         print(\"\\n\\nREPORT\")\n",
    "#         print(type(state))\n",
    "        # 這時候的 state 是一個 batch 有 32 個 frames 而不是 single image\n",
    "        state = [preprocess_frame(frame) for frame in state] # 要把 batch 中的每個 frame 都轉換\n",
    "#         print(\"\\n\\nPREPROCESSED\")\n",
    "#         print(type(state))\n",
    "        state = torch.cat(state) # 要把 python list 接成一個 pytorch tensor\n",
    "#         print(\"\\n\\nCONCATANTE\")\n",
    "#         print(type(state))\n",
    "#         print(state.shape)\n",
    "#         print(\"\\n\\n\\n\")\n",
    "        \n",
    "        new_state = [preprocess_frame(frame) for frame in new_state]\n",
    "        new_state = torch.cat(new_state)\n",
    "        \n",
    "        # 要把全部的 data 轉成 tensor\n",
    "#         state = Tensor(state).to(device)\n",
    "#         new_state = Tensor(new_state).to(device)\n",
    "        # 這時候 reward, action, done 都是 batch 而不是單一的數值\n",
    "        reward = Tensor(reward).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "        \n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach() # 用 learning NN\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1] # 用 [1] 取出 index\n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            # new_state_values = self.nn(new_state).detach()\n",
    "            new_state_values = self.target_nn(new_state).detach() # 改成用 target network 來計算 new state\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0] # 1 表示從每個 row 中選出最大值, [0] 是取出最大值的數值\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values # 當 done = 1 的時候，1-done=0 所以只會剩下 reward\n",
    "\n",
    "        # print(new_state_values)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        # predicted_value = self.nn(state)[action]\n",
    "        # print(self.nn(state).size())\n",
    "        # print(action.unsqueeze(1).size())\n",
    "        # print(self.nn(state))\n",
    "        # print(action.unsqueeze(1))\n",
    "        # print(self.nn(state).gather(1, action.unsqueeze(1))) # gather 沿著 row 方向照 action 指示的值取出來 state\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print(predicted_value.squeeze(1).size())\n",
    "        # print(target_value.size())\n",
    "        loss = self.loss_func(predicted_value, target_value) # predicted_value 和 target_value 要有相同的 dimension\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "#         if self.update_target_counter % update_target_frequency == 0:\n",
    "        if self.number_of_frames % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "#         if self.update_target_counter % save_model_frequency == 0:\n",
    "        if self.number_of_frames % save_model_frequency == 0:\n",
    "            save_model(self.nn)\n",
    "            \n",
    "#         self.update_target_counter += 1\n",
    "        self.number_of_frames += 1\n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "rewards_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "#         step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        memory.push(state, action, new_state, reward, done) # 把 experience 收集到 memory 中\n",
    "        qnet_agent.optimize()\n",
    "        # qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        \n",
    "#         print(\"IMAGE\")\n",
    "#         print(state)\n",
    "#         print(\"\\n\\nDATA TYPE\")\n",
    "#         print(type(state))\n",
    "#         print(state.dtype)\n",
    "#         print(\"\\n\\nSHAPE\")\n",
    "#         print(state.shape)\n",
    "#         print(\"\\n\\n\\n\")\n",
    "#         print(\"TRANSPOSE\")\n",
    "#         # 要把 numpy array 轉成 pytorch 能讀的格式\n",
    "#         state = state.transpose((2, 0, 1)) # (2, 0, 1) 表示 new order of dims \n",
    "#         # numpy array 中每個 col 的 index (0, 1, 2) 放到 pytorch 後要變成 (2, 0, 1)\n",
    "#         print(state.shape)\n",
    "#         print(\"\\n\\n\\n\")\n",
    "        \n",
    "        if done:\n",
    "            rewards_total.append(score)\n",
    "            mean_reward_100 = sum(rewards_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0 and i_episode > 0): # 第一個 episode 時 data 量不夠，所以要 i_episode > 0\n",
    "                plot_results()\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(rewards_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(rewards_total)/len(rewards_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(rewards_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(rewards_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.title(\"Rewards\")\n",
    "# plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "# plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9b3f8",
   "metadata": {},
   "source": [
    "* 目標是在 100 分鐘內解出來\n",
    "  * 怎樣算解出來？ 最後 100 個 episodes 的平均點數要 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b79796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from atari_wrappers import make_atari, wrap_deepmind\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "# env = gym.make(\"CartPole-v0\")\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env)\n",
    "\n",
    "# 把結果存成 video\n",
    "directory = \"./PongVideos/\"\n",
    "env = gym.wrappers.Monitor(env, directory, video_callable=lambda episode_id: episode_id % 20 == 0)\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.0001\n",
    "num_episodes = 500\n",
    "gamma = 0.99\n",
    "\n",
    "hidden_layer = 512\n",
    "\n",
    "replay_memory_size = 100000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 5000 # 每 500 steps 更新一次 target network\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 10000\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 18\n",
    "\n",
    "clip_error = True\n",
    "normalize_image = True\n",
    "\n",
    "file2save = \"pong_save.pth\"\n",
    "save_model_frequency = 10000 # 每 1 萬個 frame 就存檔一次\n",
    "resume_previous_training = False\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_output = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "def load_model():\n",
    "    return torch.load(file2save)\n",
    "\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), file2save)\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    frame = frame.transpose((2, 0, 1))\n",
    "    frame = torch.from_numpy(frame)\n",
    "    frame = frame.to(device, dtype=torch.float32)\n",
    "    frame = frame.unsqueeze(1)\n",
    "    return frame\n",
    "    \n",
    "def plot_results():\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.plot(rewards_total, alpha=0.6, color=\"red\")\n",
    "#     plt.show()\n",
    "    plt.savefig(\"Pong-results.png\")\n",
    "    plt.close()\n",
    "    \n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # memory 的大小\n",
    "        self.memory = []\n",
    "        self.position = 0 # 幫忙追蹤放到 memory 中的 entry\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done) # 把所有資訊放到一個 transition 中，然後把 transition 餵給 memory\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # 在 memory 中加入新的 transition\n",
    "        else:\n",
    "            self.memory[self.position] = transition # overwrite 原有的 memory\n",
    "            \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size): # batch_size 是只有多少 entries 要做 sampling\n",
    "        return zip(*random.sample(self.memory, batch_size)) # random.sample(從哪裡取數，要取幾個數)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "#         self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        # self.linear2 = nn.Linear(hidden_layer, number_of_output)\n",
    "        # 改用 CNN: 有三層 Conv layers 和兩層 FC\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernal_size=8, stride=4) # input 是 1 frames\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernal_size=4, stride=2) # input 是前一級的 output\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernal_size=3, stride=1)\n",
    "        \n",
    "        # 第二層改為 value 和 advantage\n",
    "        self.advantage1 = nn.Linear(7*7*64, hidden_layer)\n",
    "        self.advantage2 = nn.Linear(hidden_layer, number_of_output) # advantage 有 number of actions 個輸出\n",
    "        \n",
    "        self.value1 = nn.Linear(7*7*64, hidden_layer)\n",
    "        self.value2 = nn.Linear(hidden_layer, 1) # value 只有一個輸出\n",
    "        \n",
    "#         self.activation = nn.Tanh()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if normalize_image:\n",
    "            x = x / 255\n",
    "#         output = self.linear1(x)\n",
    "#         output1 = self.activation(output1)\n",
    "        output_conf = self.conv1(x)\n",
    "        output_conf = self.activation(output_conf)\n",
    "        output_conf = self.conv2(output_conf)\n",
    "        output_conf = self.activation(output_conf)\n",
    "        output_conf = self.conv3(output_conf)\n",
    "        output_conf = self.activation(output_conf)\n",
    "        \n",
    "        # flatten: 把 conv 的輸出轉成 2d tensor\n",
    "        output_conf = output_conf.view(output_conf.size(0), -1)\n",
    "        \n",
    "        # output2 = self.linear2(output1)\n",
    "        output_advantage = self.advantage1(output_conf)\n",
    "        output_advantage = self.activation(output_advantage)\n",
    "        output_advantage = self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value = self.value1(output_conf)\n",
    "        output_value = self.activation(output_value)\n",
    "        output_value = self.value2(output_value)\n",
    "        \n",
    "        # 要把 value 和 advantage 的輸出結合起來\n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "        \n",
    "        return output_final\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "#         self.update_target_counter = 0 # 用來追蹤什麼時候需要更新 target net\n",
    "        self.number_of_frames = 0\n",
    "        \n",
    "        if resume_previous_training and os.path.exists(file2save):\n",
    "            print(\"Loading previously saved model ...\")\n",
    "            self.nn.load_state_dict(load_model())\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = preprocess_frame(state)\n",
    "#                 state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "#                 print(\"\\n\\n\\nACTION NN\")\n",
    "#                 print(action_from_nn)\n",
    "                action = torch.max(action_from_nn, 1)[1] # 第二個參數從 0 改成 1\n",
    "#                 print(\"\\n\\nACTION\")\n",
    "#                 print(action)\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # 要取出 barch_size 這麼多個 samples, 那 memory 中必須要有 batch_size 這麼多個 samples 才行\n",
    "        if (len(memory) < batch_size):\n",
    "            return # 當 memory 比 batch_size 小的時候就直接傳回\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "#         print(\"\\n\\nREPORT\")\n",
    "#         print(type(state))\n",
    "        # 這時候的 state 是一個 batch 有 32 個 frames 而不是 single image\n",
    "        state = [preprocess_frame(frame) for frame in state] # 要把 batch 中的每個 frame 都轉換\n",
    "#         print(\"\\n\\nPREPROCESSED\")\n",
    "#         print(type(state))\n",
    "        state = torch.cat(state) # 要把 python list 接成一個 pytorch tensor\n",
    "#         print(\"\\n\\nCONCATANTE\")\n",
    "#         print(type(state))\n",
    "#         print(state.shape)\n",
    "#         print(\"\\n\\n\\n\")\n",
    "        \n",
    "        new_state = [preprocess_frame(frame) for frame in new_state]\n",
    "        new_state = torch.cat(new_state)\n",
    "        \n",
    "        # 要把全部的 data 轉成 tensor\n",
    "#         state = Tensor(state).to(device)\n",
    "#         new_state = Tensor(new_state).to(device)\n",
    "        # 這時候 reward, action, done 都是 batch 而不是單一的數值\n",
    "        reward = Tensor(reward).to(device) # reward 是 scalar 所以要改成 Tensor\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "        \n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach() # 用 learning NN\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1] # 用 [1] 取出 index\n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            # new_state_values = self.nn(new_state).detach()\n",
    "            new_state_values = self.target_nn(new_state).detach() # 改成用 target network 來計算 new state\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0] # 1 表示從每個 row 中選出最大值, [0] 是取出最大值的數值\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values # 當 done = 1 的時候，1-done=0 所以只會剩下 reward\n",
    "\n",
    "        # print(new_state_values)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        # predicted_value = self.nn(state)[action]\n",
    "        # print(self.nn(state).size())\n",
    "        # print(action.unsqueeze(1).size())\n",
    "        # print(self.nn(state))\n",
    "        # print(action.unsqueeze(1))\n",
    "        # print(self.nn(state).gather(1, action.unsqueeze(1))) # gather 沿著 row 方向照 action 指示的值取出來 state\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print(predicted_value.squeeze(1).size())\n",
    "        # print(target_value.size())\n",
    "        loss = self.loss_func(predicted_value, target_value) # predicted_value 和 target_value 要有相同的 dimension\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "#         if self.update_target_counter % update_target_frequency == 0:\n",
    "        if self.number_of_frames % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "#         if self.update_target_counter % save_model_frequency == 0:\n",
    "        if self.number_of_frames % save_model_frequency == 0:\n",
    "            save_model(self.nn)\n",
    "            \n",
    "#         self.update_target_counter += 1\n",
    "        self.number_of_frames += 1\n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "rewards_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "start_time = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "#         step += 1\n",
    "        frames_total += 1\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        memory.push(state, action, new_state, reward, done) # 把 experience 收集到 memory 中\n",
    "        qnet_agent.optimize()\n",
    "        # qnet_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        \n",
    "#         print(\"IMAGE\")\n",
    "#         print(state)\n",
    "#         print(\"\\n\\nDATA TYPE\")\n",
    "#         print(type(state))\n",
    "#         print(state.dtype)\n",
    "#         print(\"\\n\\nSHAPE\")\n",
    "#         print(state.shape)\n",
    "#         print(\"\\n\\n\\n\")\n",
    "#         print(\"TRANSPOSE\")\n",
    "#         # 要把 numpy array 轉成 pytorch 能讀的格式\n",
    "#         state = state.transpose((2, 0, 1)) # (2, 0, 1) 表示 new order of dims \n",
    "#         # numpy array 中每個 col 的 index (0, 1, 2) 放到 pytorch 後要變成 (2, 0, 1)\n",
    "#         print(state.shape)\n",
    "#         print(\"\\n\\n\\n\")\n",
    "        \n",
    "        if done:\n",
    "            rewards_total.append(score)\n",
    "            mean_reward_100 = sum(rewards_total[-100:])/100\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes\" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "                \n",
    "            if (i_episode % report_interval == 0 and i_episode > 0): # 第一個 episode 時 data 量不夠，所以要 i_episode > 0\n",
    "                plot_results()\n",
    "                print(\"\\n**** Episode %i ***\\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f,\\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\"\n",
    "                      % \n",
    "                      (i_episode,\n",
    "                       report_interval,\n",
    "                       sum(rewards_total[-report_interval:])/report_interval,\n",
    "                       mean_reward_100,\n",
    "                       sum(rewards_total)/len(rewards_total),\n",
    "                       epsilon,\n",
    "                       frames_total\n",
    "                      )\n",
    "                     )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "            break\n",
    "            \n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(rewards_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(rewards_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.title(\"Rewards\")\n",
    "# plt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color=\"green\")\n",
    "# plt.show()\n",
    "\n",
    "# 要把環境關掉\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63595f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e43063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f3cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
